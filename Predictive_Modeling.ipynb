{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff194698-b99b-4172-ba42-64d6e52b6719",
   "metadata": {},
   "source": [
    "# MD+ Datathon\n",
    "### Neuroncdocs Team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7424c43-063d-401d-8104-dfa48b720d7e",
   "metadata": {},
   "source": [
    "# Predictive Modeling\n",
    "### Inputs\n",
    "* `user_id`: User ID. There are duplicates, but the unique values are 42283 users. No passes \n",
    "\n",
    "* `age`: User's age. Based on the statistics, there are some anomalies such as the minimum age is -196691 and the maximum age is 2018. More processing or filtering may be required. There are also quite a lot of missing values 309226. It might be worth replacing them with the mean or median value\n",
    "  \n",
    "* `sex`: User gender. There are 132135 missing values. Requires conversion from a categorical variable to a numeric variable. And it is important to check why there are 4 unique values. Gaps should be replaced with the value unknown\n",
    "  \n",
    "* `country`: User's country. There are 297985 missing values. Requires conversion from a categorical variable to a numeric variable. Gaps should be replaced with the value unknown\n",
    "  \n",
    "* `checkin_date`: Tracking date. It is important to convert to datetime format for ease of use. No passes\n",
    "  \n",
    "* `trackable_id`: ID of the event being tracked. Unique values, 264603 events. It's probably possible to delete the column, but not sure yet. No passes trackable_type: The type of event to track. 7 unique types. Explore all types. No passes ('trackable_type' --> 'trackable_name')\n",
    "  - `Condition` --> condition_keyword_groups\n",
    "  - `Symptom` --> symptom_keyword_groups\n",
    "  - `Food` --> food_keyword_groups()\n",
    "  - `Tag` --> tag_keyword_groups()\n",
    "  - `Weather` --> `icon`, `temperature_min`, `temperature_max`, `precip_intensity`, `pressure`, `humidity`\n",
    "  - `HBI`\n",
    "    \n",
    "* `trackable_name`: Name of the event being tracked, description of symptoms. 117214 unique values. There are 4 gaps\n",
    "  \n",
    "* `trackable_value`: The value of the trackable event. 15960 unique values - severity for some conditions (0-4)\n",
    "\n",
    "\n",
    "### Outputs\n",
    "* `Condition`: the incidence of the disease/diagnosis of interest after 30 days\n",
    "  - `Epilepsy`, `Depression`, `Anxiety`\n",
    "\n",
    "\n",
    "### Models Explored\n",
    "1. Gradient Boosting Classifier\n",
    "2. Random Forest Classifier\n",
    "3. Logisitic Regression\n",
    "4. Recurrent Neural Network\n",
    "5. Temporal Convolutional Neural Network\n",
    "\n",
    "### Model Performance & Comparison\n",
    "* AUC\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d1d5e-befc-49a7-914d-0882528654dd",
   "metadata": {},
   "source": [
    "# Input Classification Buckets\n",
    "### Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d410fac3-d6bd-4e7b-846d-a5650d8e34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_keyword_groups = {\n",
    "\n",
    "    # --- Psychiatric & Behavioral ---\n",
    "    \"depression\": [\n",
    "        \"depression\", \"Depression\",\n",
    "        \"major depressive\", \"Major depressive\",\n",
    "        \"mdd\", \"MDD\", \"dysthymia\", \"Dysthymia\",\n",
    "        \"seasonal affective disorder\", \"Seasonal affective disorder\"\n",
    "    ],\n",
    "    \"anxiety\": [\n",
    "        \"anxiety\", \"Anxiety\",\n",
    "        \"generalized anxiety\", \"Generalized anxiety\",\n",
    "        \"gad\", \"GAD\", \"panic attack\", \"Panic attack\",\n",
    "        \"panic disorder\", \"Panic disorder\",\n",
    "        \"phobia\", \"Phobia\", \"social anxiety\", \"Social anxiety\"\n",
    "    ],\n",
    "    \"ocd\": [\n",
    "        \"ocd\", \"OCD\",\n",
    "        \"obsessive compulsive\", \"Obsessive compulsive\",\n",
    "        \"obsessive-compulsive\", \"Obsessive-compulsive\",\n",
    "        \"obsessive compulsive disorder\", \"Obsessive compulsive disorder\"\n",
    "    ],\n",
    "    \"ptsd\": [\n",
    "        \"ptsd\", \"PTSD\",\n",
    "        \"post traumatic stress\", \"Post traumatic stress\",\n",
    "        \"post-traumatic stress disorder\", \"Post-traumatic stress disorder\"\n",
    "    ],\n",
    "    \"bipolar\": [\n",
    "        \"bipolar\", \"Bipolar\",\n",
    "        \"bipolar ii\", \"Bipolar ii\", \"bipolar 2\", \"Bipolar 2\",\n",
    "        \"bipolar i\", \"Bipolar i\", \"bipolar disorder\", \"Bipolar disorder\",\n",
    "        \"cyclothymia\", \"Cyclothymia\"\n",
    "    ],\n",
    "    \"schizophrenia_psychotic\": [\n",
    "        \"schizophrenia\", \"Schizophrenia\",\n",
    "        \"psychosis\", \"Psychosis\", \"schizoaffective\", \"Schizoaffective\",\n",
    "        \"delusional disorder\", \"Delusional disorder\"\n",
    "    ],\n",
    "    \"adhd\": [\n",
    "        \"adhd\", \"ADHD\",\n",
    "        \"attention deficit\", \"Attention deficit\",\n",
    "        \"attention deficit hyperactivity\", \"Attention deficit hyperactivity\"\n",
    "    ],\n",
    "    \"substance_use\": [\n",
    "        \"substance use\", \"Substance use\",\n",
    "        \"substance abuse\", \"Substance abuse\",\n",
    "        \"alcohol use\", \"Alcohol use\",\n",
    "        \"drug abuse\", \"Drug abuse\",\n",
    "        \"opioid dependence\", \"Opioid dependence\",\n",
    "        \"addiction\", \"Addiction\", \"alcoholism\", \"Alcoholism\",\n",
    "        \"nicotine dependence\", \"Nicotine dependence\", \"cannabis use\", \"Cannabis use\"\n",
    "    ],\n",
    "    \"insomnia\": [\n",
    "        \"insomnia\", \"Insomnia\", \"sleep disorder\", \"Sleep disorder\",\n",
    "        \"circadian rhythm disorder\", \"Circadian rhythm disorder\",\n",
    "        \"restless legs\", \"Restless legs\"\n",
    "    ],\n",
    "\n",
    "    # --- Neurological ---\n",
    "    \"epilepsy_seizure\": [\n",
    "        \"seizure\", \"Seizure\", \"seizures\", \"Seizures\",\n",
    "        \"epilepsy\", \"Epilepsy\", \"epileptic\", \"Epileptic\",\n",
    "        \"convulsion\", \"Convulsion\", \"temporal lobe seizure\", \"Temporal lobe seizure\"\n",
    "    ],\n",
    "    \"migraine\": [\n",
    "        \"migraine\", \"Migraine\", \"chronic migraine\", \"Chronic migraine\",\n",
    "        \"cluster headache\", \"Cluster headache\",\n",
    "        \"tension headache\", \"Tension headache\"\n",
    "    ],\n",
    "    \"multiple_sclerosis\": [\n",
    "        \"multiple sclerosis\", \"Multiple sclerosis\", \"ms\", \"MS\"\n",
    "    ],\n",
    "    \"parkinson_disease\": [\n",
    "        \"parkinson\", \"Parkinson\", \"parkinsonism\", \"Parkinsonism\"\n",
    "    ],\n",
    "    \"neuropathy\": [\n",
    "        \"neuropathy\", \"Neuropathy\", \"peripheral neuropathy\", \"Peripheral neuropathy\",\n",
    "        \"nerve damage\", \"Nerve damage\"\n",
    "    ],\n",
    "    \"fibromyalgia\": [\n",
    "        \"fibromyalgia\", \"Fibromyalgia\", \"fibro\", \"Fibro\"\n",
    "    ],\n",
    "    \"chronic_fatigue\": [\n",
    "        \"chronic fatigue\", \"Chronic fatigue\",\n",
    "        \"cfs\", \"CFS\", \"myalgic encephalomyelitis\", \"Myalgic encephalomyelitis\",\n",
    "        \"chronic fatigue syndrome\", \"Chronic fatigue syndrome\"\n",
    "    ],\n",
    "\n",
    "    # --- Pain & Musculoskeletal ---\n",
    "    \"chronic_pain\": [\n",
    "        \"chronic pain\", \"Chronic pain\", \"pain disorder\", \"Pain disorder\",\n",
    "        \"widespread pain\", \"Widespread pain\", \"pain syndrome\", \"Pain syndrome\"\n",
    "    ],\n",
    "    \"back_pain\": [\n",
    "        \"back pain\", \"Back pain\", \"lower back pain\", \"Lower back pain\",\n",
    "        \"upper back pain\", \"Upper back pain\", \"lumbar pain\", \"Lumbar pain\",\n",
    "        \"spinal pain\", \"Spinal pain\"\n",
    "    ],\n",
    "    \"joint_pain\": [\n",
    "        \"joint pain\", \"Joint pain\", \"arthralgia\", \"Arthralgia\",\n",
    "        \"arthritis\", \"Arthritis\", \"osteoarthritis\", \"Osteoarthritis\",\n",
    "        \"rheumatoid arthritis\", \"Rheumatoid arthritis\"\n",
    "    ],\n",
    "    \"autoimmune_rheumatologic\": [\n",
    "        \"lupus\", \"Lupus\", \"sle\", \"SLE\",\n",
    "        \"sjogren\", \"Sjogren\", \"scleroderma\", \"Scleroderma\",\n",
    "        \"vasculitis\", \"Vasculitis\", \"ankylosing spondylitis\", \"Ankylosing spondylitis\",\n",
    "        \"psoriatic arthritis\", \"Psoriatic arthritis\"\n",
    "    ],\n",
    "\n",
    "    # --- Endocrine & Metabolic ---\n",
    "    \"thyroid_disorders\": [\n",
    "        \"thyroid\", \"Thyroid\", \"hypothyroid\", \"Hypothyroid\",\n",
    "        \"hyperthyroid\", \"Hyperthyroid\", \"hashimoto\", \"Hashimoto\"\n",
    "    ],\n",
    "    \"diabetes\": [\n",
    "        \"diabetes\", \"Diabetes\", \"type 1 diabetes\", \"Type 1 diabetes\",\n",
    "        \"type 2 diabetes\", \"Type 2 diabetes\"\n",
    "    ],\n",
    "    \"adrenal_pituitary\": [\n",
    "        \"addison\", \"Addison\", \"cushing\", \"Cushing\",\n",
    "        \"adrenal insufficiency\", \"Adrenal insufficiency\",\n",
    "        \"pituitary tumor\", \"Pituitary tumor\"\n",
    "    ],\n",
    "    \"pcos\": [\n",
    "        \"pcos\", \"PCOS\", \"polycystic ovary\", \"Polycystic ovary\"\n",
    "    ],\n",
    "    \"menopause_hormonal\": [\n",
    "        \"menopause\", \"Menopause\", \"perimenopause\", \"Perimenopause\",\n",
    "        \"hormone imbalance\", \"Hormone imbalance\"\n",
    "    ],\n",
    "\n",
    "    # --- Gastrointestinal & Hepatic ---\n",
    "    \"ibs\": [\n",
    "        \"ibs\", \"IBS\", \"irritable bowel\", \"Irritable bowel\"\n",
    "    ],\n",
    "    \"ibd\": [\n",
    "        \"ibd\", \"IBD\", \"crohn\", \"Crohn\",\n",
    "        \"ulcerative colitis\", \"Ulcerative colitis\"\n",
    "    ],\n",
    "    \"gastroparesis\": [\n",
    "        \"gastroparesis\", \"Gastroparesis\"\n",
    "    ],\n",
    "    \"celiac\": [\n",
    "        \"celiac\", \"Celiac\", \"gluten intolerance\", \"Gluten intolerance\"\n",
    "    ],\n",
    "    \"liver_disease\": [\n",
    "        \"hepatitis\", \"Hepatitis\", \"fatty liver\", \"Fatty liver\",\n",
    "        \"cirrhosis\", \"Cirrhosis\"\n",
    "    ],\n",
    "\n",
    "    # --- Cardiovascular & Respiratory ---\n",
    "    \"hypertension\": [\n",
    "        \"hypertension\", \"Hypertension\", \"high blood pressure\", \"High blood pressure\"\n",
    "    ],\n",
    "    \"heart_disease\": [\n",
    "        \"heart disease\", \"Heart disease\", \"arrhythmia\", \"Arrhythmia\",\n",
    "        \"atrial fibrillation\", \"Atrial fibrillation\", \"coronary artery\", \"Coronary artery\"\n",
    "    ],\n",
    "    \"pots_autonomic\": [\n",
    "        \"pots\", \"POTS\", \"postural orthostatic tachycardia\", \"Postural orthostatic tachycardia\",\n",
    "        \"autonomic dysfunction\", \"Autonomic dysfunction\"\n",
    "    ],\n",
    "    \"asthma\": [\n",
    "        \"asthma\", \"Asthma\", \"bronchial asthma\", \"Bronchial asthma\"\n",
    "    ],\n",
    "    \"copd\": [\n",
    "        \"copd\", \"COPD\", \"chronic obstructive\", \"Chronic obstructive\"\n",
    "    ],\n",
    "\n",
    "    # --- Allergy / Immunology ---\n",
    "    \"allergies\": [\n",
    "        \"allergy\", \"Allergy\", \"allergies\", \"Allergies\",\n",
    "        \"hay fever\", \"Hay fever\", \"seasonal allergy\", \"Seasonal allergy\"\n",
    "    ],\n",
    "    \"eczema_psoriasis\": [\n",
    "        \"eczema\", \"Eczema\", \"psoriasis\", \"Psoriasis\", \"dermatitis\", \"Dermatitis\"\n",
    "    ],\n",
    "    \"food_intolerance\": [\n",
    "        \"food allergy\", \"Food allergy\", \"lactose intolerance\", \"Lactose intolerance\",\n",
    "        \"gluten intolerance\", \"Gluten intolerance\"\n",
    "    ],\n",
    "\n",
    "    # --- Gynecologic / Urologic ---\n",
    "    \"endometriosis\": [\n",
    "        \"endometriosis\", \"Endometriosis\", \"pelvic pain\", \"Pelvic pain\"\n",
    "    ],\n",
    "    \"interstitial_cystitis\": [\n",
    "        \"interstitial cystitis\", \"Interstitial cystitis\", \"bladder pain\", \"Bladder pain\"\n",
    "    ],\n",
    "    \"urinary_tract_infection\": [\n",
    "        \"urinary tract infection\", \"Urinary tract infection\", \"uti\", \"UTI\"\n",
    "    ],\n",
    "\n",
    "    # --- Dermatologic ---\n",
    "    \"acne_rosacea\": [\n",
    "        \"acne\", \"Acne\", \"rosacea\", \"Rosacea\"\n",
    "    ],\n",
    "    \"alopecia\": [\n",
    "        \"alopecia\", \"Alopecia\", \"hair loss\", \"Hair loss\"\n",
    "    ],\n",
    "\n",
    "    # --- Infectious / Systemic ---\n",
    "    \"lyme_disease\": [\n",
    "        \"lyme\", \"Lyme\", \"borreliosis\", \"Borreliosis\"\n",
    "    ],\n",
    "    \"chronic_infection\": [\n",
    "        \"chronic infection\", \"Chronic infection\", \"viral infection\", \"Viral infection\",\n",
    "        \"mononucleosis\", \"Mononucleosis\", \"ebv\", \"EBV\", \"cmv\", \"CMV\"\n",
    "    ],\n",
    "    \"covid_long_covid\": [\n",
    "        \"covid\", \"Covid\", \"covid-19\", \"COVID-19\", \"long covid\", \"Long covid\", \"post covid\", \"Post covid\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17721037-6fa0-4fb2-a0d1-24a366358658",
   "metadata": {},
   "source": [
    "### Symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c7e288c-4f00-4621-a9c5-8cf3aee9a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_keyword_groups = {\n",
    "    \"negative_affect\": [\n",
    "        \"depressed\", \"Depressed\", \"depression\", \"Depression\",\n",
    "        \"sad\", \"Sad\", \"low mood\", \"Low mood\",\n",
    "        \"irritable\", \"Irritable\", \"irritability\", \"Irritability\",\n",
    "        \"hopeless\", \"Hopeless\", \"anhedonia\", \"Anhedonia\"\n",
    "    ],\n",
    "    \"anxiety_fear_panic\": [\n",
    "        \"anxiety\", \"Anxiety\", \"worried\", \"Worried\", \"worry\", \"Worry\",\n",
    "        \"panic\", \"Panic\", \"panic attack\", \"Panic attack\",\n",
    "        \"fear\", \"Fear\", \"phobia\", \"Phobia\"\n",
    "    ],\n",
    "    \"stress_tension\": [\n",
    "        \"stress\", \"Stress\", \"stressed\", \"Stressed\",\n",
    "        \"tension\", \"Tension\", \"overwhelmed\", \"Overwhelmed\"\n",
    "    ],\n",
    "    \"positive_affect_energy\": [\n",
    "        \"calm\", \"Calm\", \"relaxed\", \"Relaxed\", \"happy\", \"Happy\",\n",
    "        \"grateful\", \"Grateful\", \"energetic\", \"Energetic\"\n",
    "    ],\n",
    "    \"fatigue_exhaustion\": [\n",
    "        \"fatigue\", \"Fatigue\", \"exhausted\", \"Exhausted\",\n",
    "        \"tired\", \"Tired\", \"low energy\", \"Low energy\",\n",
    "        \"lethargy\", \"Lethargy\", \"weak\", \"Weak\"\n",
    "    ],\n",
    "    \"sleep_symptoms\": [\n",
    "        \"insomnia\", \"Insomnia\", \"difficulty sleeping\", \"Difficulty sleeping\",\n",
    "        \"poor sleep\", \"Poor sleep\", \"nightmares\", \"Nightmares\",\n",
    "        \"hypersomnia\", \"Hypersomnia\", \"restless sleep\", \"Restless sleep\"\n",
    "    ],\n",
    "    \"pain_general\": [\n",
    "        \"pain\", \"Pain\", \"ache\", \"Ache\", \"aching\", \"Aching\",\n",
    "        \"soreness\", \"Soreness\", \"discomfort\", \"Discomfort\"\n",
    "    ],\n",
    "    \"pain_headache_migraine\": [\n",
    "        \"headache\", \"Headache\", \"migraine\", \"Migraine\",\n",
    "        \"chronic migraine\", \"Chronic migraine\",\n",
    "        \"chronic daily headache\", \"Chronic daily headache\",\n",
    "        \"cluster headache\", \"Cluster headache\"\n",
    "    ],\n",
    "    \"pain_musculoskeletal\": [\n",
    "        \"joint pain\", \"Joint pain\", \"arthralgia\", \"Arthralgia\",\n",
    "        \"back pain\", \"Back pain\", \"neck pain\", \"Neck pain\",\n",
    "        \"shoulder pain\", \"Shoulder pain\", \"hip pain\", \"Hip pain\",\n",
    "        \"knee pain\", \"Knee pain\", \"tmj pain\", \"TMJ pain\",\n",
    "        \"rib pain\", \"Rib pain\", \"jaw pain\", \"Jaw pain\",\n",
    "        \"muscle soreness\", \"Muscle soreness\", \"stiffness\", \"Stiffness\"\n",
    "    ],\n",
    "    \"pain_neuropathic\": [\n",
    "        \"burning pain\", \"Burning pain\", \"tingling\", \"Tingling\",\n",
    "        \"numbness\", \"Numbness\", \"paresthesia\", \"Paresthesia\",\n",
    "        \"shooting pain\", \"Shooting pain\", \"pins and needles\", \"Pins and needles\"\n",
    "    ],\n",
    "    \"musculoskeletal_inflammation\": [\n",
    "        \"swelling\", \"Swelling\", \"redness\", \"Redness\",\n",
    "        \"inflammation\", \"Inflammation\", \"tenderness\", \"Tenderness\",\n",
    "        \"limited movement\", \"Limited movement\"\n",
    "    ],\n",
    "    \"gi_symptoms\": [\n",
    "        \"nausea\", \"Nausea\", \"vomiting\", \"Vomiting\",\n",
    "        \"diarrhea\", \"Diarrhea\", \"constipation\", \"Constipation\",\n",
    "        \"bloating\", \"Bloating\", \"abdominal pain\", \"Abdominal pain\",\n",
    "        \"cramping\", \"Cramping\", \"gas\", \"Gas\", \"flatulence\", \"Flatulence\",\n",
    "        \"reflux\", \"Reflux\", \"heartburn\", \"Heartburn\",\n",
    "        \"indigestion\", \"Indigestion\", \"IBS\", \"irritable bowel\"\n",
    "    ],\n",
    "    \"appetite_weight_symptoms\": [\n",
    "        \"loss of appetite\", \"Loss of appetite\",\n",
    "        \"increased appetite\", \"Increased appetite\",\n",
    "        \"weight gain\", \"Weight gain\", \"weight loss\", \"Weight loss\"\n",
    "    ],\n",
    "    \"respiratory_symptoms\": [\n",
    "        \"cough\", \"Cough\", \"wheeze\", \"Wheeze\",\n",
    "        \"shortness of breath\", \"Shortness of breath\",\n",
    "        \"dyspnea\", \"Dyspnea\", \"chest tightness\", \"Chest tightness\",\n",
    "        \"breathing difficulty\", \"Breathing difficulty\"\n",
    "    ],\n",
    "    \"cardiovascular_symptoms\": [\n",
    "        \"palpitations\", \"Palpitations\", \"chest pain\", \"Chest pain\",\n",
    "        \"tachycardia\", \"Tachycardia\", \"bradycardia\", \"Bradycardia\",\n",
    "        \"syncope\", \"Syncope\", \"lightheaded\", \"Lightheaded\", \"faint\", \"Faint\"\n",
    "    ],\n",
    "    \"autonomic_symptoms\": [\n",
    "        \"dizziness\", \"Dizziness\", \"orthostatic\", \"Orthostatic\",\n",
    "        \"temperature intolerance\", \"Temperature intolerance\",\n",
    "        \"hot flashes\", \"Hot flashes\", \"cold intolerance\", \"Cold intolerance\",\n",
    "        \"sweating\", \"Sweating\", \"fainting\", \"Fainting\"\n",
    "    ],\n",
    "    \"neurologic_other\": [\n",
    "        \"vertigo\", \"Vertigo\", \"tremor\", \"Tremor\",\n",
    "        \"seizure\", \"Seizure\", \"seizure aura\", \"Seizure aura\",\n",
    "        \"balance problems\", \"Balance problems\", \"coordination issues\", \"Coordination issues\"\n",
    "    ],\n",
    "    \"cognitive_symptoms\": [\n",
    "        \"brain fog\", \"Brain fog\", \"memory problems\", \"Memory problems\",\n",
    "        \"poor concentration\", \"Poor concentration\", \"confusion\", \"Confusion\",\n",
    "        \"slow thinking\", \"Slow thinking\", \"difficulty focusing\", \"Difficulty focusing\"\n",
    "    ],\n",
    "    \"dermatologic_symptoms\": [\n",
    "        \"rash\", \"Rash\", \"itch\", \"Itch\", \"itching\", \"Itching\",\n",
    "        \"hives\", \"Hives\", \"eczema\", \"Eczema\", \"psoriasis\", \"Psoriasis\",\n",
    "        \"acne\", \"Acne\", \"dry skin\", \"Dry skin\"\n",
    "    ],\n",
    "    \"hair_nail_changes\": [\n",
    "        \"hair loss\", \"Hair loss\", \"brittle nails\", \"Brittle nails\",\n",
    "        \"nail changes\", \"Nail changes\", \"thinning hair\", \"Thinning hair\"\n",
    "    ],\n",
    "    \"gyne_urologic_symptoms\": [\n",
    "        \"pelvic pain\", \"Pelvic pain\", \"dysmenorrhea\", \"Dysmenorrhea\",\n",
    "        \"urinary frequency\", \"Urinary frequency\", \"urgency\", \"Urgency\",\n",
    "        \"burning urination\", \"Burning urination\",\n",
    "        \"menstrual cramps\", \"Menstrual cramps\", \"heavy periods\", \"Heavy periods\"\n",
    "    ],\n",
    "    \"endocrine_metabolic\": [\n",
    "        \"cold intolerance\", \"Cold intolerance\",\n",
    "        \"heat intolerance\", \"Heat intolerance\",\n",
    "        \"excessive thirst\", \"Excessive thirst\", \"polyuria\", \"Polyuria\",\n",
    "        \"hypoglycemia\", \"Hypoglycemia\", \"hot flashes\", \"Hot flashes\"\n",
    "    ],\n",
    "    \"allergy_immunology\": [\n",
    "        \"sneezing\", \"Sneezing\", \"runny nose\", \"Runny nose\",\n",
    "        \"nasal congestion\", \"Nasal congestion\", \"itchy eyes\", \"Itchy eyes\",\n",
    "        \"sinus pressure\", \"Sinus pressure\", \"postnasal drip\", \"Postnasal drip\"\n",
    "    ],\n",
    "    \"ent_eye_symptoms\": [\n",
    "        \"ear pain\", \"Ear pain\", \"hearing loss\", \"Hearing loss\",\n",
    "        \"tinnitus\", \"Tinnitus\", \"sinus pain\", \"Sinus pain\",\n",
    "        \"blurred vision\", \"Blurred vision\", \"double vision\", \"Double vision\",\n",
    "        \"eye pain\", \"Eye pain\", \"dry eyes\", \"Dry eyes\"\n",
    "    ],\n",
    "    \"infectious_feverish\": [\n",
    "        \"fever\", \"Fever\", \"chills\", \"Chills\",\n",
    "        \"flu-like\", \"Flu-like\", \"sore throat\", \"Sore throat\",\n",
    "        \"infection\", \"Infection\", \"lymph node\", \"Lymph node\"\n",
    "    ],\n",
    "    \"psychotic_perceptual\": [\n",
    "        \"hallucination\", \"Hallucination\", \"delusion\", \"Delusion\",\n",
    "        \"paranoia\", \"Paranoia\", \"disorganized thinking\", \"Disorganized thinking\"\n",
    "    ],\n",
    "    \"derealization_depersonalization\": [\n",
    "        \"derealization\", \"Derealization\", \"depersonalization\", \"Depersonalization\",\n",
    "        \"out of body\", \"Out of body\", \"detached\", \"Detached\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546d0d4-d36a-44f6-b9e8-cc51540d92c8",
   "metadata": {},
   "source": [
    "### Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6de1d95c-f5a9-45c4-9910-c72731932bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Food classification buckets\n",
    "food_keyword_groups = {\n",
    "    # Vegetables\n",
    "    \"vegetables_leafy\": [\n",
    "        \"spinach\", \"Spinach\", \"kale\", \"Kale\", \"lettuce\", \"Lettuce\",\n",
    "        \"arugula\", \"Arugula\", \"chard\", \"Chard\", \"collard\", \"Collard\"\n",
    "    ],\n",
    "    \"vegetables_cruciferous\": [\n",
    "        \"broccoli\", \"Broccoli\", \"cauliflower\", \"Cauliflower\",\n",
    "        \"cabbage\", \"Cabbage\", \"brussels sprouts\", \"Brussels sprouts\"\n",
    "    ],\n",
    "    \"vegetables_root\": [\n",
    "        \"potato\", \"Potato\", \"potatoes\", \"Potatoes\", \"sweet potato\", \"Sweet potato\",\n",
    "        \"yam\", \"Yam\", \"carrot\", \"Carrot\", \"beet\", \"Beet\", \"radish\", \"Radish\"\n",
    "    ],\n",
    "    \"vegetables_nightshade\": [\n",
    "        \"tomato\", \"Tomato\", \"tomatoes\", \"Tomatoes\",\n",
    "        \"eggplant\", \"Eggplant\", \"bell pepper\", \"Bell pepper\",\n",
    "        \"pepper\", \"Pepper\"\n",
    "    ],\n",
    "    \"vegetables_allium\": [\n",
    "        \"onion\", \"Onion\", \"garlic\", \"Garlic\", \"leek\", \"Leek\", \"shallot\", \"Shallot\"\n",
    "    ],\n",
    "    \"legumes\": [\n",
    "        \"beans\", \"Beans\", \"black beans\", \"Black beans\", \"kidney beans\", \"Kidney beans\",\n",
    "        \"lentils\", \"Lentils\", \"chickpeas\", \"Chickpeas\", \"peas\", \"Peas\", \"soybeans\", \"Soybeans\", \"tofu\", \"Tofu\", \"tempeh\", \"Tempeh\"\n",
    "    ],\n",
    "\n",
    "    # Fruits\n",
    "    \"fruits_berries\": [\n",
    "        \"strawberry\", \"Strawberry\", \"blueberry\", \"Blueberry\", \"raspberry\", \"Raspberry\",\n",
    "        \"blackberry\", \"Blackberry\"\n",
    "    ],\n",
    "    \"fruits_citrus\": [\n",
    "        \"orange\", \"Orange\", \"lemon\", \"Lemon\", \"lime\", \"Lime\", \"grapefruit\", \"Grapefruit\"\n",
    "    ],\n",
    "    \"fruits_tropical\": [\n",
    "        \"banana\", \"Banana\", \"mango\", \"Mango\", \"pineapple\", \"Pineapple\",\n",
    "        \"papaya\", \"Papaya\", \"coconut\", \"Coconut\"\n",
    "    ],\n",
    "    \"fruits_pome\": [\n",
    "        \"apple\", \"Apple\", \"pear\", \"Pear\"\n",
    "    ],\n",
    "\n",
    "    # Grains / starches\n",
    "    \"grains_whole\": [\n",
    "        \"brown rice\", \"Brown rice\", \"quinoa\", \"Quinoa\", \"oats\", \"Oats\",\n",
    "        \"barley\", \"Barley\", \"farro\", \"Farro\", \"buckwheat\", \"Buckwheat\"\n",
    "    ],\n",
    "    \"grains_refined\": [\n",
    "        \"white rice\", \"White rice\", \"white bread\", \"White bread\",\n",
    "        \"pasta\", \"Pasta\", \"noodles\", \"Noodles\", \"bagel\", \"Bagel\"\n",
    "    ],\n",
    "    \"starches_root\": [\n",
    "        \"potato\", \"Potato\", \"potatoes\", \"Potatoes\", \"yams\", \"Yams\", \"sweet potato\", \"Sweet potato\", \"taro\", \"Taro\"\n",
    "    ],\n",
    "    \"gluten_items\": [\n",
    "        \"wheat\", \"Wheat\", \"barley\", \"Barley\", \"rye\", \"Rye\",\n",
    "        \"bread\", \"Bread\", \"pasta\", \"Pasta\", \"crackers\", \"Crackers\"\n",
    "    ],\n",
    "\n",
    "    # Dairy & eggs\n",
    "    \"dairy_milk\": [\n",
    "        \"milk\", \"Milk\"\n",
    "    ],\n",
    "    \"dairy_fermented\": [\n",
    "        \"yogurt\", \"Yogurt\", \"kefir\", \"Kefir\"\n",
    "    ],\n",
    "    \"dairy_cheese\": [\n",
    "        \"cheese\", \"Cheese\", \"cheddar\", \"Cheddar\", \"mozzarella\", \"Mozzarella\"\n",
    "    ],\n",
    "    \"dairy_fat\": [\n",
    "        \"butter\", \"Butter\", \"ghee\", \"Ghee\", \"cream\", \"Cream\"\n",
    "    ],\n",
    "    \"eggs\": [\n",
    "        \"egg\", \"Egg\", \"eggs\", \"Eggs\"\n",
    "    ],\n",
    "\n",
    "    # Meats & fish\n",
    "    \"meat_beef\": [\n",
    "        \"beef\", \"Beef\", \"steak\", \"Steak\", \"ground beef\", \"Ground beef\"\n",
    "    ],\n",
    "    \"meat_pork\": [\n",
    "        \"pork\", \"Pork\", \"bacon\", \"Bacon\", \"ham\", \"Ham\", \"sausage\", \"Sausage\"\n",
    "    ],\n",
    "    \"meat_chicken\": [\n",
    "        \"chicken\", \"Chicken\"\n",
    "    ],\n",
    "    \"meat_turkey\": [\n",
    "        \"turkey\", \"Turkey\"\n",
    "    ],\n",
    "    \"fish_seafood\": [\n",
    "        \"fish\", \"Fish\", \"salmon\", \"Salmon\", \"tuna\", \"Tuna\",\n",
    "        \"sardines\", \"Sardines\", \"shrimp\", \"Shrimp\"\n",
    "    ],\n",
    "\n",
    "    # Nuts, seeds, oils\n",
    "    \"nuts_seeds\": [\n",
    "        \"almond\", \"Almond\", \"almonds\", \"Almonds\", \"peanut\", \"Peanut\", \"peanuts\", \"Peanuts\",\n",
    "        \"walnut\", \"Walnut\", \"walnuts\", \"Walnuts\", \"cashew\", \"Cashew\", \"cashews\", \"Cashews\",\n",
    "        \"chia\", \"Chia\", \"flax\", \"Flax\", \"pumpkin seeds\", \"Pumpkin seeds\", \"sunflower seeds\", \"Sunflower seeds\"\n",
    "    ],\n",
    "    \"oils_fats\": [\n",
    "        \"olive oil\", \"Olive oil\", \"avocado oil\", \"Avocado oil\",\n",
    "        \"coconut oil\", \"Coconut oil\", \"canola oil\", \"Canola oil\"\n",
    "    ],\n",
    "\n",
    "    # Processed foods & sweets\n",
    "    \"processed_snacks\": [\n",
    "        \"chips\", \"Chips\", \"crisps\", \"Crisps\", \"crackers\", \"Crackers\", \"cookies\", \"Cookies\"\n",
    "    ],\n",
    "    \"processed_fastfood\": [\n",
    "        \"pizza\", \"Pizza\", \"burger\", \"Burger\", \"fries\", \"Fries\", \"fried chicken\", \"Fried chicken\",\n",
    "        \"hot dog\", \"Hot dog\"\n",
    "    ],\n",
    "    \"sugary_sweets\": [\n",
    "        \"candy\", \"Candy\", \"chocolate\", \"Chocolate\", \"dessert\", \"Dessert\",\n",
    "        \"ice cream\", \"Ice cream\", \"cake\", \"Cake\", \"donut\", \"Donut\"\n",
    "    ],\n",
    "\n",
    "    # Beverages\n",
    "    \"beverages_caffeinated\": [\n",
    "        \"coffee\", \"Coffee\", \"espresso\", \"Espresso\", \"tea\", \"Tea\", \"matcha\", \"Matcha\"\n",
    "    ],\n",
    "    \"beverages_alcohol\": [\n",
    "        \"wine\", \"Wine\", \"beer\", \"Beer\", \"liquor\", \"Liquor\", \"vodka\", \"Vodka\",\n",
    "        \"whiskey\", \"Whiskey\", \"rum\", \"Rum\"\n",
    "    ],\n",
    "    \"beverages_soda\": [\n",
    "        \"soda\", \"Soda\", \"cola\", \"Cola\", \"soft drink\", \"Soft drink\"\n",
    "    ],\n",
    "\n",
    "    # Spices & condiments\n",
    "    \"spices_condiments\": [\n",
    "        \"turmeric\", \"Turmeric\", \"ginger\", \"Ginger\", \"cinnamon\", \"Cinnamon\",\n",
    "        \"soy sauce\", \"Soy sauce\", \"ketchup\", \"Ketchup\", \"mustard\", \"Mustard\",\n",
    "        \"mayonnaise\", \"Mayonnaise\", \"hot sauce\", \"Hot sauce\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acb33c-3ca9-4590-8bbf-547525e3d83a",
   "metadata": {},
   "source": [
    "### Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ff17194-6242-4c93-bde0-d226f2e39b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment classification buckets\n",
    "treatment_keyword_groups = {\n",
    "    \"nsaids\": [\n",
    "        \"ibuprofen\", \"Ibuprofen\", \"naproxen\", \"Naproxen\", \"diclofenac\", \"Diclofenac\",\n",
    "        \"indomethacin\", \"Indomethacin\", \"meloxicam\", \"Meloxicam\", \"celecoxib\", \"Celecoxib\"\n",
    "    ],\n",
    "    \"acetaminophen\": [\n",
    "        \"acetaminophen\", \"Acetaminophen\", \"paracetamol\", \"Paracetamol\", \"tylenol\", \"Tylenol\"\n",
    "    ],\n",
    "    \"opioids\": [\n",
    "        \"oxycodone\", \"Oxycodone\", \"hydrocodone\", \"Hydrocodone\", \"tramadol\", \"Tramadol\",\n",
    "        \"morphine\", \"Morphine\", \"codeine\", \"Codeine\", \"fentanyl\", \"Fentanyl\"\n",
    "    ],\n",
    "    \"anticonvulsants\": [\n",
    "        \"gabapentin\", \"Gabapentin\", \"pregabalin\", \"Pregabalin\",\n",
    "        \"topiramate\", \"Topiramate\", \"valproate\", \"Valproate\", \"divalproex\", \"Divalproex\",\n",
    "        \"lamotrigine\", \"Lamotrigine\", \"levetiracetam\", \"Levetiracetam\",\n",
    "        \"carbamazepine\", \"Carbamazepine\"\n",
    "    ],\n",
    "    \"ssri_antidepressants\": [\n",
    "        \"fluoxetine\", \"Fluoxetine\", \"sertraline\", \"Sertraline\",\n",
    "        \"citalopram\", \"Citalopram\", \"escitalopram\", \"Escitalopram\",\n",
    "        \"paroxetine\", \"Paroxetine\", \"fluvoxamine\", \"Fluvoxamine\"\n",
    "    ],\n",
    "    \"snri_antidepressants\": [\n",
    "        \"venlafaxine\", \"Venlafaxine\", \"desvenlafaxine\", \"Desvenlafaxine\",\n",
    "        \"duloxetine\", \"Duloxetine\", \"milnacipran\", \"Milnacipran\"\n",
    "    ],\n",
    "    \"tca_antidepressants\": [\n",
    "        \"amitriptyline\", \"Amitriptyline\", \"nortriptyline\", \"Nortriptyline\",\n",
    "        \"imipramine\", \"Imipramine\", \"desipramine\", \"Desipramine\", \"clomipramine\", \"Clomipramine\"\n",
    "    ],\n",
    "    \"maoi_antidepressants\": [\n",
    "        \"phenelzine\", \"Phenelzine\", \"tranylcypromine\", \"Tranylcypromine\", \"isocarboxazid\", \"Isocarboxazid\"\n",
    "    ],\n",
    "    \"atypical_antidepressants\": [\n",
    "        \"bupropion\", \"Bupropion\", \"mirtazapine\", \"Mirtazapine\",\n",
    "        \"trazodone\", \"Trazodone\"\n",
    "    ],\n",
    "    \"benzodiazepines\": [\n",
    "        \"lorazepam\", \"Lorazepam\", \"clonazepam\", \"Clonazepam\",\n",
    "        \"diazepam\", \"Diazepam\", \"alprazolam\", \"Alprazolam\"\n",
    "    ],\n",
    "    \"beta_blockers\": [\n",
    "        \"propranolol\", \"Propranolol\", \"metoprolol\", \"Metoprolol\",\n",
    "        \"atenolol\", \"Atenolol\", \"nadolol\", \"Nadolol\"\n",
    "    ],\n",
    "    \"calcium_channel_blockers\": [\n",
    "        \"verapamil\", \"Verapamil\", \"diltiazem\", \"Diltiazem\"\n",
    "    ],\n",
    "    \"antipsychotics\": [\n",
    "        \"quetiapine\", \"Quetiapine\", \"olanzapine\", \"Olanzapine\",\n",
    "        \"risperidone\", \"Risperidone\", \"aripiprazole\", \"Aripiprazole\",\n",
    "        \"ziprasidone\", \"Ziprasidone\"\n",
    "    ],\n",
    "    \"stimulants_adhd\": [\n",
    "        \"methylphenidate\", \"Methylphenidate\",\n",
    "        \"amphetamine\", \"Amphetamine\", \"dextroamphetamine\", \"Dextroamphetamine\",\n",
    "        \"lisdexamfetamine\", \"Lisdexamfetamine\"\n",
    "    ],\n",
    "    \"sleep_agents\": [\n",
    "        \"zolpidem\", \"Zolpidem\", \"eszopiclone\", \"Eszopiclone\",\n",
    "        \"zopiclone\", \"Zopiclone\", \"suvorexant\", \"Suvorexant\",\n",
    "        \"ramelteon\", \"Ramelteon\", \"melatonin\", \"Melatonin\"\n",
    "    ],\n",
    "    \"muscle_relaxants\": [\n",
    "        \"cyclobenzaprine\", \"Cyclobenzaprine\", \"tizanidine\", \"Tizanidine\",\n",
    "        \"baclofen\", \"Baclofen\", \"methocarbamol\", \"Methocarbamol\"\n",
    "    ],\n",
    "    \"anti_cgrp_migraine\": [\n",
    "        \"erenumab\", \"Erenumab\", \"fremanezumab\", \"Fremanezumab\",\n",
    "        \"galcanezumab\", \"Galcanezumab\", \"eptinezumab\", \"Eptinezumab\",\n",
    "        \"rimegepant\", \"Rimegepant\", \"ubrogepant\", \"Ubrogepant\", \"atogepant\", \"Atogepant\"\n",
    "    ],\n",
    "    \"triptans\": [\n",
    "        \"sumatriptan\", \"Sumatriptan\", \"rizatriptan\", \"Rizatriptan\",\n",
    "        \"eletriptan\", \"Eletriptan\", \"zolmitriptan\", \"Zolmitriptan\"\n",
    "    ],\n",
    "    \"steroids\": [\n",
    "        \"prednisone\", \"Prednisone\", \"prednisolone\", \"Prednisolone\",\n",
    "        \"methylprednisolone\", \"Methylprednisolone\", \"dexamethasone\", \"Dexamethasone\",\n",
    "        \"budesonide\", \"Budesonide\"\n",
    "    ],\n",
    "    \"dmard_immunosuppressants\": [\n",
    "        \"methotrexate\", \"Methotrexate\", \"azathioprine\", \"Azathioprine\",\n",
    "        \"hydroxychloroquine\", \"Hydroxychloroquine\", \"sulfasalazine\", \"Sulfasalazine\",\n",
    "        \"leflunomide\", \"Leflunomide\"\n",
    "    ],\n",
    "    \"biologics\": [\n",
    "        \"adalimumab\", \"Adalimumab\", \"infliximab\", \"Infliximab\",\n",
    "        \"etanercept\", \"Etanercept\", \"ustekinumab\", \"Ustekinumab\",\n",
    "        \"secukinumab\", \"Secukinumab\"\n",
    "    ],\n",
    "    \"pots_medications\": [\n",
    "        \"fludrocortisone\", \"Fludrocortisone\", \"midodrine\", \"Midodrine\",\n",
    "        \"ivabradine\", \"Ivabradine\"\n",
    "    ],\n",
    "    \"gi_acid_reducers\": [\n",
    "        \"omeprazole\", \"Omeprazole\", \"lansoprazole\", \"Lansoprazole\",\n",
    "        \"pantoprazole\", \"Pantoprazole\", \"esomeprazole\", \"Esomeprazole\",\n",
    "        \"famotidine\", \"Famotidine\", \"ranitidine\", \"Ranitidine\"\n",
    "    ],\n",
    "    \"antiemetics\": [\n",
    "        \"ondansetron\", \"Ondansetron\", \"metoclopramide\", \"Metoclopramide\",\n",
    "        \"promethazine\", \"Promethazine\", \"prochlorperazine\", \"Prochlorperazine\"\n",
    "    ],\n",
    "    \"laxatives\": [\n",
    "        \"polyethylene glycol\", \"Polyethylene glycol\", \"miralax\", \"Miralax\",\n",
    "        \"senna\", \"Senna\", \"bisacodyl\", \"Bisacodyl\"\n",
    "    ],\n",
    "    \"antidiarrheals\": [\n",
    "        \"loperamide\", \"Loperamide\", \"imodium\", \"Imodium\"\n",
    "    ],\n",
    "    \"supplements\": [\n",
    "        \"vitamin d\", \"Vitamin D\", \"magnesium\", \"Magnesium\",\n",
    "        \"vitamin b12\", \"Vitamin B12\", \"iron\", \"Iron\",\n",
    "        \"omega-3\", \"Omega-3\", \"fish oil\", \"Fish oil\",\n",
    "        \"turmeric\", \"Turmeric\", \"curcumin\", \"Curcumin\",\n",
    "        \"coq10\", \"CoQ10\", \"probiotic\", \"Probiotic\"\n",
    "    ],\n",
    "    \"nonpharm_therapy\": [\n",
    "        \"cbt\", \"CBT\", \"psychotherapy\", \"Psychotherapy\", \"therapy\", \"Therapy\",\n",
    "        \"mindfulness\", \"Mindfulness\", \"meditation\", \"Meditation\",\n",
    "        \"yoga\", \"Yoga\", \"acupuncture\", \"Acupuncture\",\n",
    "        \"physical therapy\", \"Physical therapy\", \"physiotherapy\", \"Physiotherapy\",\n",
    "        \"massage\", \"Massage\", \"chiropractic\", \"Chiropractic\"\n",
    "    ],\n",
    "    \"hormonal_agents\": [\n",
    "        \"oral contraceptive\", \"Oral contraceptive\", \"birth control\", \"Birth control\",\n",
    "        \"estrogen\", \"Estrogen\", \"progesterone\", \"Progesterone\",\n",
    "        \"levonorgestrel\", \"Levonorgestrel\", \"iud\", \"IUD\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0fa00-207b-4359-824d-ad6c3a0b322e",
   "metadata": {},
   "source": [
    "### Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "997c469a-e453-4ac0-afde-bdb940eb027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag classification buckets\n",
    "\n",
    "tag_keyword_groups = {\n",
    "    \"stress_emotional\": [\n",
    "        \"stress\", \"Stress\", \"stressed\", \"Stressed\", \"overwhelmed\", \"Overwhelmed\",\n",
    "        \"argument\", \"Argument\", \"conflict\", \"Conflict\", \"anxiety\", \"Anxiety\"\n",
    "    ],\n",
    "    \"sleep_quality\": [\n",
    "        \"poor sleep\", \"Poor sleep\", \"insomnia\", \"Insomnia\",\n",
    "        \"sleep deprivation\", \"Sleep deprivation\", \"overslept\", \"Overslept\",\n",
    "        \"nap\", \"Nap\", \"night shift\", \"Night shift\", \"jet lag\", \"Jet lag\"\n",
    "    ],\n",
    "    \"menstrual_cycle\": [\n",
    "        \"period\", \"Period\", \"menstruation\", \"Menstruation\",\n",
    "        \"pms\", \"PMS\", \"cramps\", \"Cramps\", \"ovulation\", \"Ovulation\"\n",
    "    ],\n",
    "    \"physical_activity\": [\n",
    "        \"exercise\", \"Exercise\", \"workout\", \"Workout\",\n",
    "        \"running\", \"Running\", \"walk\", \"Walk\", \"walking\", \"Walking\",\n",
    "        \"yoga\", \"Yoga\", \"strength training\", \"Strength training\"\n",
    "    ],\n",
    "    \"illness_infection\": [\n",
    "        \"sick\", \"Sick\", \"cold\", \"Cold\", \"flu\", \"Flu\",\n",
    "        \"infection\", \"Infection\", \"virus\", \"Virus\"\n",
    "    ],\n",
    "    \"weather_environment\": [\n",
    "        \"heat\", \"Heat\", \"hot\", \"Hot\", \"cold\", \"Cold\",\n",
    "        \"humidity\", \"Humidity\", \"humid\", \"Humid\",\n",
    "        \"rain\", \"Rain\", \"storm\", \"Storm\",\n",
    "        \"barometric pressure\", \"Barometric pressure\"\n",
    "    ],\n",
    "    \"dietary_triggers\": [\n",
    "        \"sugar\", \"Sugar\", \"sugary\", \"Sugary\",\n",
    "        \"caffeine\", \"Caffeine\", \"coffee\", \"Coffee\",\n",
    "        \"alcohol\", \"Alcohol\", \"gluten\", \"Gluten\",\n",
    "        \"dairy\", \"Dairy\", \"spicy\", \"Spicy\",\n",
    "        \"processed food\", \"Processed food\", \"fast food\", \"Fast food\"\n",
    "    ],\n",
    "    \"hydration\": [\n",
    "        \"dehydration\", \"Dehydration\", \"low water\", \"Low water\",\n",
    "        \"not enough water\", \"Not enough water\"\n",
    "    ],\n",
    "    \"work_school\": [\n",
    "        \"work\", \"Work\", \"deadline\", \"Deadline\",\n",
    "        \"school\", \"School\", \"exam\", \"Exam\"\n",
    "    ],\n",
    "    \"travel_schedule\": [\n",
    "        \"travel\", \"Travel\", \"jet lag\", \"Jet lag\",\n",
    "        \"flight\", \"Flight\", \"time zone\", \"Time zone\"\n",
    "    ],\n",
    "    \"allergens_irritants\": [\n",
    "        \"pollen\", \"Pollen\", \"dust\", \"Dust\",\n",
    "        \"pet dander\", \"Pet dander\", \"smoke\", \"Smoke\",\n",
    "        \"pollution\", \"Pollution\", \"fragrance\", \"Fragrance\", \"perfume\", \"Perfume\"\n",
    "    ],\n",
    "    \"medication_adherence\": [\n",
    "        \"missed dose\", \"Missed dose\", \"skipped dose\", \"Skipped dose\",\n",
    "        \"new medication\", \"New medication\", \"dose change\", \"Dose change\"\n",
    "    ],\n",
    "    \"screen_time_posture\": [\n",
    "        \"screen time\", \"Screen time\", \"computer\", \"Computer\",\n",
    "        \"phone\", \"Phone\", \"posture\", \"Posture\"\n",
    "    ],\n",
    "    \"meals_timing\": [\n",
    "        \"skipped meal\", \"Skipped meal\", \"late dinner\", \"Late dinner\",\n",
    "        \"fasting\", \"Fasting\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873aa7ab-5970-4414-b97a-271adc8cac22",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c1222-cd09-428a-a7e1-98ece0eab5d2",
   "metadata": {},
   "source": [
    "### Predicting Onset of Epilepsy after 30 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e793bef5-9bf2-4fed-9ece-7cae17725a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 62, number of negative: 112097\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 963\n",
      "[LightGBM] [Info] Number of data points in the train set: 112159, number of used features: 68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "AUC  : 0.705\n",
      "AUPRC: 0.000\n",
      "Threshold~0.05: Precision=0.000  Recall=0.000\n",
      "Threshold~0.10: Precision=0.000  Recall=0.000\n",
      "Threshold~0.20: Precision=0.000  Recall=0.000\n",
      "\n",
      "Top 25 features:\n",
      " cond__max_severity__mean_30d    6185\n",
      "age                             5815\n",
      "cond__max_severity              3704\n",
      "cond__max_severity__mean_7d     2698\n",
      "cond__depression__sum_30d       1665\n",
      "cond__anxiety__sum_30d          1625\n",
      "cond__anxiety                   1427\n",
      "country_top_GB                  1165\n",
      "cond__bipolar                   1160\n",
      "cond__ptsd__sum_30d             1127\n",
      "cond__migraine                   949\n",
      "cond__depression__sum_7d         878\n",
      "cond__depression                 873\n",
      "country_top_US                   855\n",
      "cond__migraine__sum_30d          841\n",
      "cond__ptsd                       736\n",
      "cond__anxiety__sum_7d            674\n",
      "cond__insomnia__sum_30d          543\n",
      "sex_female                       535\n",
      "cond__fibromyalgia__sum_30d      465\n",
      "cond__fibromyalgia               446\n",
      "cond__ptsd__sum_7d               431\n",
      "cond__chronic_fatigue            342\n",
      "cond__insomnia                   333\n",
      "cond__migraine__sum_7d           305\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 1) LOAD & CLEAN =========\n",
    "# Assumes your full long-format table is in df with the columns you listed\n",
    "path = \"/Users/cristybanuelos/Downloads/Chronic_Illness_Dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df[\"trackable_type\"] == \"Condition\"].copy()\n",
    "df[\"condition_clean\"] = (\n",
    "    df[\"trackable_name\"].str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Parse date\n",
    "df[\"checkin_date\"] = pd.to_datetime(df[\"checkin_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"checkin_date\"])  # keep only rows with a date\n",
    "\n",
    "# Basic user-level fields\n",
    "# Age cleaning: clip to a sensible window; set out-of-range to NaN, then impute\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "df.loc[(df[\"age\"] < 0) | (df[\"age\"] > 110), \"age\"] = np.nan  # clip biologically plausible ages\n",
    "df[\"age\"] = df.groupby(\"user_id\")[\"age\"].transform(lambda s: s.fillna(s.median()))\n",
    "df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "# Sex cleaning: normalize categories\n",
    "def norm_sex(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in {\"male\", \"m\"}: return \"male\"\n",
    "    if x in {\"female\", \"f\"}: return \"female\"\n",
    "    if x in {\"nan\", \"none\", \"\", \"unknown\"}: return \"unknown\"\n",
    "    return \"other\"\n",
    "df[\"sex\"] = df[\"sex\"].apply(norm_sex)\n",
    "\n",
    "# Country cleaning\n",
    "def norm_country(x):\n",
    "    x = str(x).strip()\n",
    "    return \"unknown\" if (x == \"\" or x.lower() == \"nan\") else x\n",
    "df[\"country\"] = df[\"country\"].apply(norm_country)\n",
    "\n",
    "# ========= 2) TEXT NORMALIZATION (for matching) =========\n",
    "# Create a clean text column for matching on trackable_name\n",
    "df[\"name_clean\"] = (\n",
    "    df[\"trackable_name\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# ========= 3) HELPERS TO FLAG KEYWORDS =========\n",
    "def add_keyword_flags(sub_df, groups_dict, prefix):\n",
    "    \"\"\"\n",
    "    For a subset of df (e.g., only Symptoms, only Food...), add binary columns\n",
    "    indicating whether trackable_name matches any keyword group for that row.\n",
    "    We then aggregate to daily features later.\n",
    "    \"\"\"\n",
    "    out = sub_df.copy()\n",
    "    for group, kws in groups_dict.items():\n",
    "        # Prebuild a regex OR pattern for speed; escape non-alnum safely\n",
    "        pattern = r\"(\" + \"|\".join([re.escape(k.lower()) for k in kws]) + r\")\"\n",
    "        col = f\"{prefix}__{group}\"\n",
    "        out[col] = out[\"name_clean\"].str.contains(pattern, regex=True).astype(int)\n",
    "    return out\n",
    "\n",
    "# ========= 4) PER-TYPE KEYWORD FLAGS =========\n",
    "# NOTE: You already have: keyword_groups (conditions incl. \"epilepsy_seizure\"),\n",
    "#       symptom_keyword_groups, food_keyword_groups, tag_keyword_groups, treatment_keyword_groups\n",
    "\n",
    "# Conditions (Condition rows only)\n",
    "cond_rows = df[df[\"trackable_type\"] == \"Condition\"]\n",
    "cond_rows = add_keyword_flags(cond_rows, condition_keyword_groups, \"cond\")\n",
    "\n",
    "# Symptoms\n",
    "sym_rows = df[df[\"trackable_type\"] == \"Symptom\"]\n",
    "sym_rows = add_keyword_flags(sym_rows, symptom_keyword_groups, \"sym\")\n",
    "\n",
    "# Food\n",
    "food_rows = df[df[\"trackable_type\"] == \"Food\"]\n",
    "food_rows = add_keyword_flags(food_rows, food_keyword_groups, \"food\")\n",
    "\n",
    "# Tags (triggers)\n",
    "tag_rows = df[df[\"trackable_type\"] == \"Tag\"]\n",
    "tag_rows = add_keyword_flags(tag_rows, tag_keyword_groups, \"tag\")\n",
    "\n",
    "# Treatments\n",
    "trt_rows = df[df[\"trackable_type\"] == \"Treatment\"]\n",
    "trt_rows = add_keyword_flags(trt_rows, treatment_keyword_groups, \"trt\")\n",
    "\n",
    "# Weather (already numeric columns; keep as-is if present)\n",
    "weather_rows = df[df[\"trackable_type\"] == \"Weather\"].copy()\n",
    "# Example expected numeric weather columns (adjust to your schema if needed)\n",
    "for col in [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]:\n",
    "    if col in weather_rows.columns:\n",
    "        weather_rows[col] = pd.to_numeric(weather_rows[col], errors=\"coerce\")\n",
    "\n",
    "# ========= 5) DAILY AGGREGATION (no leakage) =========\n",
    "# We build daily features per user, then later roll 7/30d windows that use ONLY past data.\n",
    "\n",
    "def daily_agg_flags(sub, prefix):\n",
    "    # get only the flag columns\n",
    "    flag_cols = [c for c in sub.columns if c.startswith(prefix + \"__\")]\n",
    "    if not flag_cols:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "    # include severity if available (0-4); well take max per day\n",
    "    if \"trackable_value\" in sub.columns:\n",
    "        sub[\"severity_val\"] = pd.to_numeric(sub[\"trackable_value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        sub[\"severity_val\"] = np.nan\n",
    "\n",
    "    agg = (\n",
    "        sub.groupby([\"user_id\", \"checkin_date\"])\n",
    "           .agg({**{c: \"max\" for c in flag_cols}, \"severity_val\": \"max\"})\n",
    "           .reset_index()\n",
    "    )\n",
    "    # rename severity\n",
    "    if \"severity_val\" in agg.columns:\n",
    "        agg = agg.rename(columns={\"severity_val\": f\"{prefix}__max_severity\"})\n",
    "    return agg\n",
    "\n",
    "daily_cond = daily_agg_flags(cond_rows,  \"cond\")\n",
    "daily_sym  = daily_agg_flags(sym_rows,   \"sym\")\n",
    "daily_food = daily_agg_flags(food_rows,  \"food\")\n",
    "daily_tag  = daily_agg_flags(tag_rows,   \"tag\")\n",
    "daily_trt  = daily_agg_flags(trt_rows,   \"trt\")\n",
    "\n",
    "# Daily weather (mean if multiple in same day)\n",
    "if not weather_rows.empty:\n",
    "    wcols = [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]\n",
    "    wcols = [c for c in wcols if c in weather_rows.columns]\n",
    "    daily_wx = (\n",
    "        weather_rows.groupby([\"user_id\", \"checkin_date\"])[wcols].mean().reset_index()\n",
    "    )\n",
    "else:\n",
    "    daily_wx = pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "\n",
    "# ========= Combine into one daily table =========\n",
    "# Start from all user-day keys present in any table\n",
    "parts = [daily_cond, daily_sym, daily_food, daily_tag, daily_trt, daily_wx]\n",
    "daily = None\n",
    "for p in parts:\n",
    "    if p is None or p.empty: \n",
    "        continue\n",
    "    daily = p if daily is None else pd.merge(daily, p, on=[\"user_id\", \"checkin_date\"], how=\"outer\")\n",
    "\n",
    "if daily is None:\n",
    "    raise ValueError(\"No daily features built. Check your inputs.\")\n",
    "\n",
    "daily = daily.sort_values([\"user_id\", \"checkin_date\"]).reset_index(drop=True)\n",
    "daily = daily.fillna(0)  # for flags; numeric weather stays 0 if missing (fine for tree models)\n",
    "\n",
    "# ========= 6) ROLLING (PAST-ONLY) FEATURES =========\n",
    "# For each user, compute 7d/30d rolling sums of flags + rolling means of severities & weather.\n",
    "def add_rollups(g):\n",
    "    g = g.set_index(\"checkin_date\").sort_index()\n",
    "    # rolling windows (closed='left' to use ONLY past)\n",
    "    win_defs = {\"7d\":\"7D\", \"30d\":\"30D\"}\n",
    "    for col in g.columns:\n",
    "        if col.startswith((\"cond__\", \"sym__\", \"food__\", \"tag__\", \"trt__\")) and col.endswith(\"__max_severity\") is False:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__sum_{k}\"] = g[col].rolling(win, closed=\"left\").sum()\n",
    "        # severities & weather: rolling mean\n",
    "        if col.endswith(\"__max_severity\") or col in [\"temperature_min\",\"temperature_max\",\"precip_intensity\",\"pressure\",\"humidity\"]:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__mean_{k}\"] = g[col].rolling(win, closed=\"left\").mean()\n",
    "    return g.reset_index()\n",
    "\n",
    "daily = daily.groupby(\"user_id\", group_keys=False).apply(add_rollups)\n",
    "# Fill remaining NaNs from leading window edges\n",
    "daily = daily.fillna(0)\n",
    "\n",
    "# ========= 7) BUILD THE TARGET: FIRST-EVER EPILEPSY ONSET IN NEXT 30 DAYS =========\n",
    "# Find first date epilepsy is reported per user (Condition rows matched to epilepsy group)\n",
    "epi_flag_cols = [c for c in daily.columns if c.startswith(\"cond__epilepsy_seizure\")]\n",
    "if not epi_flag_cols:\n",
    "    raise ValueError(\"No epilepsy condition flag found. Ensure 'epilepsy_seizure' exists in keyword_groups for Conditions.\")\n",
    "\n",
    "# Daily presence of epilepsy (same-day)\n",
    "daily[\"has_epilepsy_today\"] = daily[epi_flag_cols].max(axis=1)\n",
    "\n",
    "# First-ever epilepsy date\n",
    "first_epi = (\n",
    "    daily[daily[\"has_epilepsy_today\"] > 0]\n",
    "    .groupby(\"user_id\", as_index=False)[\"checkin_date\"].min()\n",
    "    .rename(columns={\"checkin_date\":\"first_epilepsy_date\"})\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "daily = daily.merge(first_epi, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Labeling:\n",
    "# On a given day t (index row), y=1 if first epilepsy date falls in (t, t+30] AND user has not had epilepsy before t.\n",
    "daily[\"label_onset_30d\"] = 0\n",
    "mask_has_future = (~daily[\"first_epilepsy_date\"].isna())\n",
    "daily.loc[mask_has_future, \"label_onset_30d\"] = (\n",
    "    (daily[\"first_epilepsy_date\"] > daily[\"checkin_date\"]) &\n",
    "    (daily[\"first_epilepsy_date\"] <= daily[\"checkin_date\"] + pd.Timedelta(days=30))\n",
    ").astype(int)\n",
    "\n",
    "# Exclude rows ON/AFTER first epilepsy (were predicting onset, not after it happens)\n",
    "daily = daily[(daily[\"first_epilepsy_date\"].isna()) | (daily[\"checkin_date\"] < daily[\"first_epilepsy_date\"])]\n",
    "\n",
    "# Optional: require a minimum history window (e.g., at least 30 past days) to form features\n",
    "daily[\"has_30d_history\"] = daily.groupby(\"user_id\")[\"checkin_date\"].transform(\n",
    "    lambda s: (s - s.min()) >= pd.Timedelta(days=30)\n",
    ")\n",
    "daily = daily[daily[\"has_30d_history\"]]\n",
    "\n",
    "# ========= 8) ADD DEMOGRAPHICS (static) =========\n",
    "# Build a static per-user table (age/sex/country) at any row; then merge with daily\n",
    "demo = df.drop_duplicates(\"user_id\")[[\"user_id\",\"age\",\"sex\",\"country\"]].copy()\n",
    "daily = daily.merge(demo, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# One-hot encode sex & country (country can be many; consider top-K and bucket rest as 'other')\n",
    "# Keep top 20 countries to control dimensionality\n",
    "top_countries = df[\"country\"].value_counts().head(20).index\n",
    "daily[\"country_top\"] = daily[\"country\"].where(daily[\"country\"].isin(top_countries), \"other\")\n",
    "\n",
    "X_cat = pd.get_dummies(daily[[\"sex\",\"country_top\"]], drop_first=False, dtype=int)\n",
    "daily = pd.concat([daily.drop(columns=[\"sex\",\"country\",\"country_top\"]), X_cat], axis=1)\n",
    "\n",
    "# ========= 9) TRAIN / TEST TEMPORAL SPLIT =========\n",
    "# Choose a cutoff date (e.g., last year as test). Adjust to your range.\n",
    "cutoff = daily[\"checkin_date\"].quantile(0.8)  # 80% oldest for train, 20% most recent for test\n",
    "train = daily[daily[\"checkin_date\"] <= cutoff].copy()\n",
    "test  = daily[daily[\"checkin_date\"] >  cutoff].copy()\n",
    "\n",
    "# Features: use all engineered columns except identifiers and leakage columns\n",
    "drop_cols = {\n",
    "    \"user_id\", \"checkin_date\", \"first_epilepsy_date\",\n",
    "    \"has_epilepsy_today\", \"has_30d_history\", \"label_onset_30d\"\n",
    "}\n",
    "feature_cols = [c for c in daily.columns if c not in drop_cols]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"label_onset_30d\"].astype(int)\n",
    "X_test  = test[feature_cols]\n",
    "y_test  = test[\"label_onset_30d\"].astype(int)\n",
    "\n",
    "# Optional: scale numeric continuous columns (LightGBM doesnt require it)\n",
    "\n",
    "# ========= 10) TRAIN LIGHTGBM =========\n",
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ========= 11) EVALUATE =========\n",
    "p_test = clf.predict_proba(X_test)[:,1]\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "auprc = average_precision_score(y_test, p_test)\n",
    "\n",
    "print(f\"AUC  : {auc:.3f}\")\n",
    "print(f\"AUPRC: {auprc:.3f}\")\n",
    "\n",
    "# Optional: show precision/recall at a few thresholds\n",
    "prec, rec, thr = precision_recall_curve(y_test, p_test)\n",
    "for t in [0.05, 0.10, 0.20]:\n",
    "    # nearest threshold\n",
    "    idx = (np.abs(thr - t)).argmin() if len(thr) else -1\n",
    "    if idx >= 0 and idx < len(prec):\n",
    "        print(f\"Threshold~{t:0.2f}: Precision={prec[idx]:.3f}  Recall={rec[idx]:.3f}\")\n",
    "\n",
    "# ========= 12) FEATURE IMPORTANCE =========\n",
    "imp = pd.Series(clf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"\\nTop 25 features:\\n\", imp.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f95ca6-8e9b-4787-b0ef-1390e1716bb1",
   "metadata": {},
   "source": [
    "### Predicting Onset of Depression after 30 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "02d22466-30f5-4be2-afb6-71af814395d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 496, number of negative: 83290\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 958\n",
      "[LightGBM] [Info] Number of data points in the train set: 83786, number of used features: 68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "AUC  : 0.510\n",
      "AUPRC: 0.003\n",
      "Threshold~0.05: Precision=0.000  Recall=0.000\n",
      "Threshold~0.10: Precision=0.000  Recall=0.000\n",
      "Threshold~0.20: Precision=0.000  Recall=0.000\n",
      "\n",
      "Top 25 features:\n",
      " age                               12853\n",
      "cond__max_severity__mean_30d      10879\n",
      "cond__max_severity__mean_7d        6121\n",
      "cond__max_severity                 4265\n",
      "country_top_US                     1881\n",
      "cond__migraine__sum_30d            1839\n",
      "cond__fibromyalgia__sum_30d        1837\n",
      "cond__chronic_fatigue__sum_30d     1587\n",
      "country_top_GB                     1233\n",
      "cond__fibromyalgia                 1201\n",
      "cond__anxiety__sum_30d             1145\n",
      "country_top_CA                     1102\n",
      "country_top_AU                     1018\n",
      "cond__chronic_fatigue               896\n",
      "cond__joint_pain__sum_30d           876\n",
      "cond__anxiety                       869\n",
      "cond__migraine                      790\n",
      "sex_male                            766\n",
      "cond__chronic_fatigue__sum_7d       753\n",
      "sex_female                          700\n",
      "cond__insomnia__sum_30d             669\n",
      "cond__ptsd__sum_30d                 631\n",
      "country_top_unknown                 563\n",
      "cond__migraine__sum_7d              561\n",
      "cond__chronic_pain                  559\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 1) LOAD & CLEAN =========\n",
    "# Assumes your full long-format table is in df with the columns you listed\n",
    "path = \"/Users/cristybanuelos/Downloads/Chronic_Illness_Dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df[\"trackable_type\"] == \"Condition\"].copy()\n",
    "df[\"condition_clean\"] = (\n",
    "    df[\"trackable_name\"].str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Parse date\n",
    "df[\"checkin_date\"] = pd.to_datetime(df[\"checkin_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"checkin_date\"])  # keep only rows with a date\n",
    "\n",
    "# Basic user-level fields\n",
    "# Age cleaning: clip to a sensible window; set out-of-range to NaN, then impute\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "df.loc[(df[\"age\"] < 0) | (df[\"age\"] > 110), \"age\"] = np.nan  # clip biologically plausible ages\n",
    "df[\"age\"] = df.groupby(\"user_id\")[\"age\"].transform(lambda s: s.fillna(s.median()))\n",
    "df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "# Sex cleaning: normalize categories\n",
    "def norm_sex(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in {\"male\", \"m\"}: return \"male\"\n",
    "    if x in {\"female\", \"f\"}: return \"female\"\n",
    "    if x in {\"nan\", \"none\", \"\", \"unknown\"}: return \"unknown\"\n",
    "    return \"other\"\n",
    "df[\"sex\"] = df[\"sex\"].apply(norm_sex)\n",
    "\n",
    "# Country cleaning\n",
    "def norm_country(x):\n",
    "    x = str(x).strip()\n",
    "    return \"unknown\" if (x == \"\" or x.lower() == \"nan\") else x\n",
    "df[\"country\"] = df[\"country\"].apply(norm_country)\n",
    "\n",
    "# ========= 2) TEXT NORMALIZATION (for matching) =========\n",
    "# Create a clean text column for matching on trackable_name\n",
    "df[\"name_clean\"] = (\n",
    "    df[\"trackable_name\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# ========= 3) HELPERS TO FLAG KEYWORDS =========\n",
    "def add_keyword_flags(sub_df, groups_dict, prefix):\n",
    "    \"\"\"\n",
    "    For a subset of df (e.g., only Symptoms, only Food...), add binary columns\n",
    "    indicating whether trackable_name matches any keyword group for that row.\n",
    "    We then aggregate to daily features later.\n",
    "    \"\"\"\n",
    "    out = sub_df.copy()\n",
    "    for group, kws in groups_dict.items():\n",
    "        # Prebuild a regex OR pattern for speed; escape non-alnum safely\n",
    "        pattern = r\"(\" + \"|\".join([re.escape(k.lower()) for k in kws]) + r\")\"\n",
    "        col = f\"{prefix}__{group}\"\n",
    "        out[col] = out[\"name_clean\"].str.contains(pattern, regex=True).astype(int)\n",
    "    return out\n",
    "\n",
    "# ========= 4) PER-TYPE KEYWORD FLAGS =========\n",
    "# NOTE: You already have: keyword_groups (conditions incl. \"epilepsy_seizure\"),\n",
    "#       symptom_keyword_groups, food_keyword_groups, tag_keyword_groups, treatment_keyword_groups\n",
    "\n",
    "# Conditions (Condition rows only)\n",
    "cond_rows = df[df[\"trackable_type\"] == \"Condition\"]\n",
    "cond_rows = add_keyword_flags(cond_rows, condition_keyword_groups, \"cond\")\n",
    "\n",
    "# Symptoms\n",
    "sym_rows = df[df[\"trackable_type\"] == \"Symptom\"]\n",
    "sym_rows = add_keyword_flags(sym_rows, symptom_keyword_groups, \"sym\")\n",
    "\n",
    "# Food\n",
    "food_rows = df[df[\"trackable_type\"] == \"Food\"]\n",
    "food_rows = add_keyword_flags(food_rows, food_keyword_groups, \"food\")\n",
    "\n",
    "# Tags (triggers)\n",
    "tag_rows = df[df[\"trackable_type\"] == \"Tag\"]\n",
    "tag_rows = add_keyword_flags(tag_rows, tag_keyword_groups, \"tag\")\n",
    "\n",
    "# Treatments\n",
    "trt_rows = df[df[\"trackable_type\"] == \"Treatment\"]\n",
    "trt_rows = add_keyword_flags(trt_rows, treatment_keyword_groups, \"trt\")\n",
    "\n",
    "# Weather (already numeric columns; keep as-is if present)\n",
    "weather_rows = df[df[\"trackable_type\"] == \"Weather\"].copy()\n",
    "# Example expected numeric weather columns (adjust to your schema if needed)\n",
    "for col in [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]:\n",
    "    if col in weather_rows.columns:\n",
    "        weather_rows[col] = pd.to_numeric(weather_rows[col], errors=\"coerce\")\n",
    "\n",
    "# ========= 5) DAILY AGGREGATION (no leakage) =========\n",
    "# We build daily features per user, then later roll 7/30d windows that use ONLY past data.\n",
    "\n",
    "def daily_agg_flags(sub, prefix):\n",
    "    # get only the flag columns\n",
    "    flag_cols = [c for c in sub.columns if c.startswith(prefix + \"__\")]\n",
    "    if not flag_cols:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "    # include severity if available (0-4); well take max per day\n",
    "    if \"trackable_value\" in sub.columns:\n",
    "        sub[\"severity_val\"] = pd.to_numeric(sub[\"trackable_value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        sub[\"severity_val\"] = np.nan\n",
    "\n",
    "    agg = (\n",
    "        sub.groupby([\"user_id\", \"checkin_date\"])\n",
    "           .agg({**{c: \"max\" for c in flag_cols}, \"severity_val\": \"max\"})\n",
    "           .reset_index()\n",
    "    )\n",
    "    # rename severity\n",
    "    if \"severity_val\" in agg.columns:\n",
    "        agg = agg.rename(columns={\"severity_val\": f\"{prefix}__max_severity\"})\n",
    "    return agg\n",
    "\n",
    "daily_cond = daily_agg_flags(cond_rows,  \"cond\")\n",
    "daily_sym  = daily_agg_flags(sym_rows,   \"sym\")\n",
    "daily_food = daily_agg_flags(food_rows,  \"food\")\n",
    "daily_tag  = daily_agg_flags(tag_rows,   \"tag\")\n",
    "daily_trt  = daily_agg_flags(trt_rows,   \"trt\")\n",
    "\n",
    "# Daily weather (mean if multiple in same day)\n",
    "if not weather_rows.empty:\n",
    "    wcols = [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]\n",
    "    wcols = [c for c in wcols if c in weather_rows.columns]\n",
    "    daily_wx = (\n",
    "        weather_rows.groupby([\"user_id\", \"checkin_date\"])[wcols].mean().reset_index()\n",
    "    )\n",
    "else:\n",
    "    daily_wx = pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "\n",
    "# ========= Combine into one daily table =========\n",
    "# Start from all user-day keys present in any table\n",
    "parts = [daily_cond, daily_sym, daily_food, daily_tag, daily_trt, daily_wx]\n",
    "daily = None\n",
    "for p in parts:\n",
    "    if p is None or p.empty: \n",
    "        continue\n",
    "    daily = p if daily is None else pd.merge(daily, p, on=[\"user_id\", \"checkin_date\"], how=\"outer\")\n",
    "\n",
    "if daily is None:\n",
    "    raise ValueError(\"No daily features built. Check your inputs.\")\n",
    "\n",
    "daily = daily.sort_values([\"user_id\", \"checkin_date\"]).reset_index(drop=True)\n",
    "daily = daily.fillna(0)  # for flags; numeric weather stays 0 if missing (fine for tree models)\n",
    "\n",
    "# ========= 6) ROLLING (PAST-ONLY) FEATURES =========\n",
    "# For each user, compute 7d/30d rolling sums of flags + rolling means of severities & weather.\n",
    "def add_rollups(g):\n",
    "    g = g.set_index(\"checkin_date\").sort_index()\n",
    "    # rolling windows (closed='left' to use ONLY past)\n",
    "    win_defs = {\"7d\":\"7D\", \"30d\":\"30D\"}\n",
    "    for col in g.columns:\n",
    "        if col.startswith((\"cond__\", \"sym__\", \"food__\", \"tag__\", \"trt__\")) and col.endswith(\"__max_severity\") is False:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__sum_{k}\"] = g[col].rolling(win, closed=\"left\").sum()\n",
    "        # severities & weather: rolling mean\n",
    "        if col.endswith(\"__max_severity\") or col in [\"temperature_min\",\"temperature_max\",\"precip_intensity\",\"pressure\",\"humidity\"]:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__mean_{k}\"] = g[col].rolling(win, closed=\"left\").mean()\n",
    "    return g.reset_index()\n",
    "\n",
    "daily = daily.groupby(\"user_id\", group_keys=False).apply(add_rollups)\n",
    "# Fill remaining NaNs from leading window edges\n",
    "daily = daily.fillna(0)\n",
    "\n",
    "# ========= 7) BUILD THE TARGET: FIRST-EVER DEPRESSION ONSET IN NEXT 30 DAYS =========\n",
    "# Find first date depression is reported per user (Condition rows matched to depression group)\n",
    "dep_flag_cols = [c for c in daily.columns if c.startswith(\"cond__depression\")]\n",
    "if not dep_flag_cols:\n",
    "    raise ValueError(\"No depression condition flag found. Ensure 'depression' exists in keyword_groups for Conditions.\")\n",
    "\n",
    "# Daily presence of depression (same-day)\n",
    "daily[\"has_depression_today\"] = daily[dep_flag_cols].max(axis=1)\n",
    "\n",
    "# First-ever depression date per user\n",
    "first_dep = (\n",
    "    daily[daily[\"has_depression_today\"] > 0]\n",
    "    .groupby(\"user_id\", as_index=False)[\"checkin_date\"].min()\n",
    "    .rename(columns={\"checkin_date\": \"first_depression_date\"})\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "daily = daily.merge(first_dep, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Labeling:\n",
    "# On a given day t (index row), y=1 if first depression date falls in (t, t+30] AND user has not had depression before t.\n",
    "daily[\"label_onset_30d\"] = 0\n",
    "mask_has_future = (~daily[\"first_depression_date\"].isna())\n",
    "daily.loc[mask_has_future, \"label_onset_30d\"] = (\n",
    "    (daily[\"first_depression_date\"] > daily[\"checkin_date\"]) &\n",
    "    (daily[\"first_depression_date\"] <= daily[\"checkin_date\"] + \n",
    "       pd.Timedelta(days=30))\n",
    "    ).astype(int)\n",
    "\n",
    "# Exclude rows ON/AFTER first depression (predict onset only)\n",
    "daily = daily[\n",
    "    (daily[\"first_depression_date\"].isna())\n",
    "    | (daily[\"checkin_date\"] < daily[\"first_depression_date\"])\n",
    "]\n",
    "\n",
    "# Optional: require a minimum history window (e.g., at least 30 past days)\n",
    "daily[\"has_30d_history\"] = daily.groupby(\"user_id\")[\"checkin_date\"].transform(\n",
    "    lambda s: (s - s.min()) >= pd.Timedelta(days=30)\n",
    ")\n",
    "daily = daily[daily[\"has_30d_history\"]]\n",
    "\n",
    "\n",
    "# ========= 8) ADD DEMOGRAPHICS (static) =========\n",
    "# Build a static per-user table (age/sex/country) at any row; then merge with daily\n",
    "demo = df.drop_duplicates(\"user_id\")[[\"user_id\",\"age\",\"sex\",\"country\"]].copy()\n",
    "daily = daily.merge(demo, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# One-hot encode sex & country (country can be many; consider top-K and bucket rest as 'other')\n",
    "# Keep top 20 countries to control dimensionality\n",
    "top_countries = df[\"country\"].value_counts().head(20).index\n",
    "daily[\"country_top\"] = daily[\"country\"].where(daily[\"country\"].isin(top_countries), \"other\")\n",
    "\n",
    "X_cat = pd.get_dummies(daily[[\"sex\",\"country_top\"]], drop_first=False, dtype=int)\n",
    "daily = pd.concat([daily.drop(columns=[\"sex\",\"country\",\"country_top\"]), X_cat], axis=1)\n",
    "\n",
    "# ========= 9) TRAIN / TEST TEMPORAL SPLIT =========\n",
    "# Choose a cutoff date (e.g., last year as test). Adjust to your range.\n",
    "cutoff = daily[\"checkin_date\"].quantile(0.8)  # 80% oldest for train, 20% most recent for test\n",
    "train = daily[daily[\"checkin_date\"] <= cutoff].copy()\n",
    "test  = daily[daily[\"checkin_date\"] >  cutoff].copy()\n",
    "\n",
    "# Features: use all engineered columns except identifiers and leakage columns\n",
    "drop_cols = {\n",
    "    \"user_id\", \"checkin_date\", \"first_depression_date\",\n",
    "    \"has_depression_today\", \"has_30d_history\", \"label_onset_30d\"\n",
    "}\n",
    "feature_cols = [c for c in daily.columns if c not in drop_cols]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"label_onset_30d\"].astype(int)\n",
    "X_test  = test[feature_cols]\n",
    "y_test  = test[\"label_onset_30d\"].astype(int)\n",
    "\n",
    "# Optional: scale numeric continuous columns (LightGBM doesnt require it)\n",
    "\n",
    "# ========= 10) TRAIN LIGHTGBM =========\n",
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ========= 11) EVALUATE =========\n",
    "p_test = clf.predict_proba(X_test)[:,1]\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "auprc = average_precision_score(y_test, p_test)\n",
    "\n",
    "print(f\"AUC  : {auc:.3f}\")\n",
    "print(f\"AUPRC: {auprc:.3f}\")\n",
    "\n",
    "# Optional: show precision/recall at a few thresholds\n",
    "prec, rec, thr = precision_recall_curve(y_test, p_test)\n",
    "for t in [0.05, 0.10, 0.20]:\n",
    "    # nearest threshold\n",
    "    idx = (np.abs(thr - t)).argmin() if len(thr) else -1\n",
    "    if idx >= 0 and idx < len(prec):\n",
    "        print(f\"Threshold~{t:0.2f}: Precision={prec[idx]:.3f}  Recall={rec[idx]:.3f}\")\n",
    "\n",
    "# ========= 12) FEATURE IMPORTANCE =========\n",
    "imp = pd.Series(clf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"\\nTop 25 features:\\n\", imp.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91995afa-8fad-4ebf-ab94-3f392983afd5",
   "metadata": {},
   "source": [
    "### Predicting Onset of Anxiety after 30 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a91e708-5be0-4e18-847d-27b9f75347fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 531, number of negative: 80678\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 951\n",
      "[LightGBM] [Info] Number of data points in the train set: 81209, number of used features: 68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "AUC  : 0.569\n",
      "AUPRC: 0.004\n",
      "Threshold~0.05: Precision=0.009  Recall=0.024\n",
      "Threshold~0.10: Precision=0.015  Recall=0.024\n",
      "Threshold~0.20: Precision=0.000  Recall=0.000\n",
      "\n",
      "Top 25 features:\n",
      " age                               11816\n",
      "cond__max_severity__mean_30d      11661\n",
      "cond__max_severity__mean_7d        6584\n",
      "cond__max_severity                 4406\n",
      "country_top_US                     1742\n",
      "cond__chronic_fatigue__sum_30d     1642\n",
      "cond__fibromyalgia__sum_30d        1640\n",
      "cond__migraine__sum_30d            1308\n",
      "country_top_GB                     1105\n",
      "cond__fibromyalgia                 1012\n",
      "cond__joint_pain__sum_30d           968\n",
      "sex_male                            949\n",
      "cond__joint_pain                    937\n",
      "cond__migraine                      925\n",
      "cond__chronic_fatigue               895\n",
      "country_top_AU                      891\n",
      "country_top_CA                      812\n",
      "cond__chronic_fatigue__sum_7d       764\n",
      "cond__depression                    752\n",
      "sex_female                          735\n",
      "cond__chronic_pain__sum_30d         717\n",
      "cond__fibromyalgia__sum_7d          699\n",
      "cond__chronic_pain                  628\n",
      "country_top_unknown                 563\n",
      "cond__back_pain__sum_30d            528\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 1) LOAD & CLEAN =========\n",
    "# Assumes your full long-format table is in df with the columns you listed\n",
    "path = \"/Users/cristybanuelos/Downloads/Chronic_Illness_Dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df[\"trackable_type\"] == \"Condition\"].copy()\n",
    "df[\"condition_clean\"] = (\n",
    "    df[\"trackable_name\"].str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Parse date\n",
    "df[\"checkin_date\"] = pd.to_datetime(df[\"checkin_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"checkin_date\"])  # keep only rows with a date\n",
    "\n",
    "# Basic user-level fields\n",
    "# Age cleaning: clip to a sensible window; set out-of-range to NaN, then impute\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "df.loc[(df[\"age\"] < 0) | (df[\"age\"] > 110), \"age\"] = np.nan  # clip biologically plausible ages\n",
    "df[\"age\"] = df.groupby(\"user_id\")[\"age\"].transform(lambda s: s.fillna(s.median()))\n",
    "df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "# Sex cleaning: normalize categories\n",
    "def norm_sex(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in {\"male\", \"m\"}: return \"male\"\n",
    "    if x in {\"female\", \"f\"}: return \"female\"\n",
    "    if x in {\"nan\", \"none\", \"\", \"unknown\"}: return \"unknown\"\n",
    "    return \"other\"\n",
    "df[\"sex\"] = df[\"sex\"].apply(norm_sex)\n",
    "\n",
    "# Country cleaning\n",
    "def norm_country(x):\n",
    "    x = str(x).strip()\n",
    "    return \"unknown\" if (x == \"\" or x.lower() == \"nan\") else x\n",
    "df[\"country\"] = df[\"country\"].apply(norm_country)\n",
    "\n",
    "# ========= 2) TEXT NORMALIZATION (for matching) =========\n",
    "# Create a clean text column for matching on trackable_name\n",
    "df[\"name_clean\"] = (\n",
    "    df[\"trackable_name\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# ========= 3) HELPERS TO FLAG KEYWORDS =========\n",
    "def add_keyword_flags(sub_df, groups_dict, prefix):\n",
    "    \"\"\"\n",
    "    For a subset of df (e.g., only Symptoms, only Food...), add binary columns\n",
    "    indicating whether trackable_name matches any keyword group for that row.\n",
    "    We then aggregate to daily features later.\n",
    "    \"\"\"\n",
    "    out = sub_df.copy()\n",
    "    for group, kws in groups_dict.items():\n",
    "        # Prebuild a regex OR pattern for speed; escape non-alnum safely\n",
    "        pattern = r\"(\" + \"|\".join([re.escape(k.lower()) for k in kws]) + r\")\"\n",
    "        col = f\"{prefix}__{group}\"\n",
    "        out[col] = out[\"name_clean\"].str.contains(pattern, regex=True).astype(int)\n",
    "    return out\n",
    "\n",
    "# ========= 4) PER-TYPE KEYWORD FLAGS =========\n",
    "# NOTE: You already have: keyword_groups (conditions incl. \"epilepsy_seizure\"),\n",
    "#       symptom_keyword_groups, food_keyword_groups, tag_keyword_groups, treatment_keyword_groups\n",
    "\n",
    "# Conditions (Condition rows only)\n",
    "cond_rows = df[df[\"trackable_type\"] == \"Condition\"]\n",
    "cond_rows = add_keyword_flags(cond_rows, condition_keyword_groups, \"cond\")\n",
    "\n",
    "# Symptoms\n",
    "sym_rows = df[df[\"trackable_type\"] == \"Symptom\"]\n",
    "sym_rows = add_keyword_flags(sym_rows, symptom_keyword_groups, \"sym\")\n",
    "\n",
    "# Food\n",
    "food_rows = df[df[\"trackable_type\"] == \"Food\"]\n",
    "food_rows = add_keyword_flags(food_rows, food_keyword_groups, \"food\")\n",
    "\n",
    "# Tags (triggers)\n",
    "tag_rows = df[df[\"trackable_type\"] == \"Tag\"]\n",
    "tag_rows = add_keyword_flags(tag_rows, tag_keyword_groups, \"tag\")\n",
    "\n",
    "# Treatments\n",
    "trt_rows = df[df[\"trackable_type\"] == \"Treatment\"]\n",
    "trt_rows = add_keyword_flags(trt_rows, treatment_keyword_groups, \"trt\")\n",
    "\n",
    "# Weather (already numeric columns; keep as-is if present)\n",
    "weather_rows = df[df[\"trackable_type\"] == \"Weather\"].copy()\n",
    "# Example expected numeric weather columns (adjust to your schema if needed)\n",
    "for col in [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]:\n",
    "    if col in weather_rows.columns:\n",
    "        weather_rows[col] = pd.to_numeric(weather_rows[col], errors=\"coerce\")\n",
    "\n",
    "# ========= 5) DAILY AGGREGATION (no leakage) =========\n",
    "# We build daily features per user, then later roll 7/30d windows that use ONLY past data.\n",
    "\n",
    "def daily_agg_flags(sub, prefix):\n",
    "    # get only the flag columns\n",
    "    flag_cols = [c for c in sub.columns if c.startswith(prefix + \"__\")]\n",
    "    if not flag_cols:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "    # include severity if available (0-4); well take max per day\n",
    "    if \"trackable_value\" in sub.columns:\n",
    "        sub[\"severity_val\"] = pd.to_numeric(sub[\"trackable_value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        sub[\"severity_val\"] = np.nan\n",
    "\n",
    "    agg = (\n",
    "        sub.groupby([\"user_id\", \"checkin_date\"])\n",
    "           .agg({**{c: \"max\" for c in flag_cols}, \"severity_val\": \"max\"})\n",
    "           .reset_index()\n",
    "    )\n",
    "    # rename severity\n",
    "    if \"severity_val\" in agg.columns:\n",
    "        agg = agg.rename(columns={\"severity_val\": f\"{prefix}__max_severity\"})\n",
    "    return agg\n",
    "\n",
    "daily_cond = daily_agg_flags(cond_rows,  \"cond\")\n",
    "daily_sym  = daily_agg_flags(sym_rows,   \"sym\")\n",
    "daily_food = daily_agg_flags(food_rows,  \"food\")\n",
    "daily_tag  = daily_agg_flags(tag_rows,   \"tag\")\n",
    "daily_trt  = daily_agg_flags(trt_rows,   \"trt\")\n",
    "\n",
    "# Daily weather (mean if multiple in same day)\n",
    "if not weather_rows.empty:\n",
    "    wcols = [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]\n",
    "    wcols = [c for c in wcols if c in weather_rows.columns]\n",
    "    daily_wx = (\n",
    "        weather_rows.groupby([\"user_id\", \"checkin_date\"])[wcols].mean().reset_index()\n",
    "    )\n",
    "else:\n",
    "    daily_wx = pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "\n",
    "# ========= Combine into one daily table =========\n",
    "# Start from all user-day keys present in any table\n",
    "parts = [daily_cond, daily_sym, daily_food, daily_tag, daily_trt, daily_wx]\n",
    "daily = None\n",
    "for p in parts:\n",
    "    if p is None or p.empty: \n",
    "        continue\n",
    "    daily = p if daily is None else pd.merge(daily, p, on=[\"user_id\", \"checkin_date\"], how=\"outer\")\n",
    "\n",
    "if daily is None:\n",
    "    raise ValueError(\"No daily features built. Check your inputs.\")\n",
    "\n",
    "daily = daily.sort_values([\"user_id\", \"checkin_date\"]).reset_index(drop=True)\n",
    "daily = daily.fillna(0)  # for flags; numeric weather stays 0 if missing (fine for tree models)\n",
    "\n",
    "# ========= 6) ROLLING (PAST-ONLY) FEATURES =========\n",
    "# For each user, compute 7d/30d rolling sums of flags + rolling means of severities & weather.\n",
    "def add_rollups(g):\n",
    "    g = g.set_index(\"checkin_date\").sort_index()\n",
    "    # rolling windows (closed='left' to use ONLY past)\n",
    "    win_defs = {\"7d\":\"7D\", \"30d\":\"30D\"}\n",
    "    for col in g.columns:\n",
    "        if col.startswith((\"cond__\", \"sym__\", \"food__\", \"tag__\", \"trt__\")) and col.endswith(\"__max_severity\") is False:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__sum_{k}\"] = g[col].rolling(win, closed=\"left\").sum()\n",
    "        # severities & weather: rolling mean\n",
    "        if col.endswith(\"__max_severity\") or col in [\"temperature_min\",\"temperature_max\",\"precip_intensity\",\"pressure\",\"humidity\"]:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__mean_{k}\"] = g[col].rolling(win, closed=\"left\").mean()\n",
    "    return g.reset_index()\n",
    "\n",
    "daily = daily.groupby(\"user_id\", group_keys=False).apply(add_rollups)\n",
    "# Fill remaining NaNs from leading window edges\n",
    "daily = daily.fillna(0)\n",
    "\n",
    "# ========= 7) BUILD THE TARGET: FIRST-EVER ANXIETY ONSET IN NEXT 30 DAYS =========\n",
    "# Find first date anxiety is reported per user (Condition rows matched to anxiety group)\n",
    "anx_flag_cols = [c for c in daily.columns if c.startswith(\"cond__anxiety\")]\n",
    "if not anx_flag_cols:\n",
    "    raise ValueError(\"No anxiety condition flag found. Ensure 'anxiety' exists in keyword_groups for Conditions.\")\n",
    "\n",
    "# Daily presence of anxiety (same-day)\n",
    "daily[\"has_anxiety_today\"] = daily[anx_flag_cols].max(axis=1)\n",
    "\n",
    "# First-ever anxiety date per user\n",
    "first_anx = (\n",
    "    daily[daily[\"has_anxiety_today\"] > 0]\n",
    "    .groupby(\"user_id\", as_index=False)[\"checkin_date\"].min()\n",
    "    .rename(columns={\"checkin_date\": \"first_anxiety_date\"})\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "daily = daily.merge(first_anx, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Labeling:\n",
    "# On a given day t (index row), y=1 if first anxiety date falls in (t, t+30] AND user has not had anxiety before t.\n",
    "daily[\"label_onset_30d\"] = 0\n",
    "mask_has_future = (~daily[\"first_anxiety_date\"].isna())\n",
    "daily.loc[mask_has_future, \"label_onset_30d\"] = (\n",
    "    (daily[\"first_anxiety_date\"] > daily[\"checkin_date\"]) &\n",
    "    (daily[\"first_anxiety_date\"] <= daily[\"checkin_date\"] + \n",
    "       pd.Timedelta(days=30))\n",
    "    ).astype(int)\n",
    "\n",
    "# Exclude rows ON/AFTER first anxiety (predict onset only)\n",
    "daily = daily[\n",
    "    (daily[\"first_anxiety_date\"].isna())\n",
    "    | (daily[\"checkin_date\"] < daily[\"first_anxiety_date\"])\n",
    "]\n",
    "\n",
    "# Optional: require a minimum history window (e.g., at least 30 past days)\n",
    "daily[\"has_30d_history\"] = daily.groupby(\"user_id\")[\"checkin_date\"].transform(\n",
    "    lambda s: (s - s.min()) >= pd.Timedelta(days=30)\n",
    ")\n",
    "daily = daily[daily[\"has_30d_history\"]]\n",
    "\n",
    "\n",
    "# ========= 8) ADD DEMOGRAPHICS (static) =========\n",
    "# Build a static per-user table (age/sex/country) at any row; then merge with daily\n",
    "demo = df.drop_duplicates(\"user_id\")[[\"user_id\",\"age\",\"sex\",\"country\"]].copy()\n",
    "daily = daily.merge(demo, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# One-hot encode sex & country (country can be many; consider top-K and bucket rest as 'other')\n",
    "# Keep top 20 countries to control dimensionality\n",
    "top_countries = df[\"country\"].value_counts().head(20).index\n",
    "daily[\"country_top\"] = daily[\"country\"].where(daily[\"country\"].isin(top_countries), \"other\")\n",
    "\n",
    "X_cat = pd.get_dummies(daily[[\"sex\",\"country_top\"]], drop_first=False, dtype=int)\n",
    "daily = pd.concat([daily.drop(columns=[\"sex\",\"country\",\"country_top\"]), X_cat], axis=1)\n",
    "\n",
    "# ========= 9) TRAIN / TEST TEMPORAL SPLIT =========\n",
    "# Choose a cutoff date (e.g., last year as test). Adjust to your range.\n",
    "cutoff = daily[\"checkin_date\"].quantile(0.8)  # 80% oldest for train, 20% most recent for test\n",
    "train = daily[daily[\"checkin_date\"] <= cutoff].copy()\n",
    "test  = daily[daily[\"checkin_date\"] >  cutoff].copy()\n",
    "\n",
    "# Features: use all engineered columns except identifiers and leakage columns\n",
    "drop_cols = {\n",
    "    \"user_id\", \"checkin_date\", \"first_anxiety_date\",\n",
    "    \"has_anxiety_today\", \"has_30d_history\", \"label_onset_30d\"\n",
    "}\n",
    "feature_cols = [c for c in daily.columns if c not in drop_cols]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"label_onset_30d\"].astype(int)\n",
    "X_test  = test[feature_cols]\n",
    "y_test  = test[\"label_onset_30d\"].astype(int)\n",
    "\n",
    "# Optional: scale numeric continuous columns (LightGBM doesnt require it)\n",
    "\n",
    "# ========= 10) TRAIN LIGHTGBM =========\n",
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ========= 11) EVALUATE =========\n",
    "p_test = clf.predict_proba(X_test)[:,1]\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "auprc = average_precision_score(y_test, p_test)\n",
    "\n",
    "print(f\"AUC  : {auc:.3f}\")\n",
    "print(f\"AUPRC: {auprc:.3f}\")\n",
    "\n",
    "# Optional: show precision/recall at a few thresholds\n",
    "prec, rec, thr = precision_recall_curve(y_test, p_test)\n",
    "for t in [0.05, 0.10, 0.20]:\n",
    "    # nearest threshold\n",
    "    idx = (np.abs(thr - t)).argmin() if len(thr) else -1\n",
    "    if idx >= 0 and idx < len(prec):\n",
    "        print(f\"Threshold~{t:0.2f}: Precision={prec[idx]:.3f}  Recall={rec[idx]:.3f}\")\n",
    "\n",
    "# ========= 12) FEATURE IMPORTANCE =========\n",
    "imp = pd.Series(clf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"\\nTop 25 features:\\n\", imp.head(25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5464f-f86b-4b18-96f2-086c93dceeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SETUP ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 1) LOAD & CLEAN =========\n",
    "# Assumes your full long-format table is in df with the columns you listed\n",
    "path = \"/Users/cristybanuelos/Downloads/Chronic_Illness_Dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df[\"trackable_type\"] == \"Condition\"].copy()\n",
    "df[\"condition_clean\"] = (\n",
    "    df[\"trackable_name\"].str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Parse date\n",
    "df[\"checkin_date\"] = pd.to_datetime(df[\"checkin_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"checkin_date\"])  # keep only rows with a date\n",
    "\n",
    "# Basic user-level fields\n",
    "# Age cleaning: clip to a sensible window; set out-of-range to NaN, then impute\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "df.loc[(df[\"age\"] < 0) | (df[\"age\"] > 110), \"age\"] = np.nan  # clip biologically plausible ages\n",
    "df[\"age\"] = df.groupby(\"user_id\")[\"age\"].transform(lambda s: s.fillna(s.median()))\n",
    "df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "# Sex cleaning: normalize categories\n",
    "def norm_sex(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in {\"male\", \"m\"}: return \"male\"\n",
    "    if x in {\"female\", \"f\"}: return \"female\"\n",
    "    if x in {\"nan\", \"none\", \"\", \"unknown\"}: return \"unknown\"\n",
    "    return \"other\"\n",
    "df[\"sex\"] = df[\"sex\"].apply(norm_sex)\n",
    "\n",
    "# Country cleaning\n",
    "def norm_country(x):\n",
    "    x = str(x).strip()\n",
    "    return \"unknown\" if (x == \"\" or x.lower() == \"nan\") else x\n",
    "df[\"country\"] = df[\"country\"].apply(norm_country)\n",
    "\n",
    "# ========= 2) TEXT NORMALIZATION (for matching) =========\n",
    "# Create a clean text column for matching on trackable_name\n",
    "df[\"name_clean\"] = (\n",
    "    df[\"trackable_name\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# ========= 3) HELPERS TO FLAG KEYWORDS =========\n",
    "def add_keyword_flags(sub_df, groups_dict, prefix):\n",
    "    \"\"\"\n",
    "    For a subset of df (e.g., only Symptoms, only Food...), add binary columns\n",
    "    indicating whether trackable_name matches any keyword group for that row.\n",
    "    We then aggregate to daily features later.\n",
    "    \"\"\"\n",
    "    out = sub_df.copy()\n",
    "    for group, kws in groups_dict.items():\n",
    "        # Prebuild a regex OR pattern for speed; escape non-alnum safely\n",
    "        pattern = r\"(\" + \"|\".join([re.escape(k.lower()) for k in kws]) + r\")\"\n",
    "        col = f\"{prefix}__{group}\"\n",
    "        out[col] = out[\"name_clean\"].str.contains(pattern, regex=True).astype(int)\n",
    "    return out\n",
    "\n",
    "# ========= 4) PER-TYPE KEYWORD FLAGS =========\n",
    "# NOTE: You already have: keyword_groups (conditions incl. \"epilepsy_seizure\"),\n",
    "#       symptom_keyword_groups, food_keyword_groups, tag_keyword_groups, treatment_keyword_groups\n",
    "\n",
    "# Conditions (Condition rows only)\n",
    "cond_rows = df[df[\"trackable_type\"] == \"Condition\"]\n",
    "cond_rows = add_keyword_flags(cond_rows, condition_keyword_groups, \"cond\")\n",
    "\n",
    "# Symptoms\n",
    "sym_rows = df[df[\"trackable_type\"] == \"Symptom\"]\n",
    "sym_rows = add_keyword_flags(sym_rows, symptom_keyword_groups, \"sym\")\n",
    "\n",
    "# Food\n",
    "food_rows = df[df[\"trackable_type\"] == \"Food\"]\n",
    "food_rows = add_keyword_flags(food_rows, food_keyword_groups, \"food\")\n",
    "\n",
    "# Tags (triggers)\n",
    "tag_rows = df[df[\"trackable_type\"] == \"Tag\"]\n",
    "tag_rows = add_keyword_flags(tag_rows, tag_keyword_groups, \"tag\")\n",
    "\n",
    "# Treatments\n",
    "trt_rows = df[df[\"trackable_type\"] == \"Treatment\"]\n",
    "trt_rows = add_keyword_flags(trt_rows, treatment_keyword_groups, \"trt\")\n",
    "\n",
    "# Weather (already numeric columns; keep as-is if present)\n",
    "weather_rows = df[df[\"trackable_type\"] == \"Weather\"].copy()\n",
    "# Example expected numeric weather columns (adjust to your schema if needed)\n",
    "for col in [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]:\n",
    "    if col in weather_rows.columns:\n",
    "        weather_rows[col] = pd.to_numeric(weather_rows[col], errors=\"coerce\")\n",
    "\n",
    "# ========= 5) DAILY AGGREGATION (no leakage) =========\n",
    "# We build daily features per user, then later roll 7/30d windows that use ONLY past data.\n",
    "\n",
    "def daily_agg_flags(sub, prefix):\n",
    "    # get only the flag columns\n",
    "    flag_cols = [c for c in sub.columns if c.startswith(prefix + \"__\")]\n",
    "    if not flag_cols:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "    # include severity if available (0-4); well take max per day\n",
    "    if \"trackable_value\" in sub.columns:\n",
    "        sub[\"severity_val\"] = pd.to_numeric(sub[\"trackable_value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        sub[\"severity_val\"] = np.nan\n",
    "\n",
    "    agg = (\n",
    "        sub.groupby([\"user_id\", \"checkin_date\"])\n",
    "           .agg({**{c: \"max\" for c in flag_cols}, \"severity_val\": \"max\"})\n",
    "           .reset_index()\n",
    "    )\n",
    "    # rename severity\n",
    "    if \"severity_val\" in agg.columns:\n",
    "        agg = agg.rename(columns={\"severity_val\": f\"{prefix}__max_severity\"})\n",
    "    return agg\n",
    "\n",
    "daily_cond = daily_agg_flags(cond_rows,  \"cond\")\n",
    "daily_sym  = daily_agg_flags(sym_rows,   \"sym\")\n",
    "daily_food = daily_agg_flags(food_rows,  \"food\")\n",
    "daily_tag  = daily_agg_flags(tag_rows,   \"tag\")\n",
    "daily_trt  = daily_agg_flags(trt_rows,   \"trt\")\n",
    "\n",
    "# Daily weather (mean if multiple in same day)\n",
    "if not weather_rows.empty:\n",
    "    wcols = [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]\n",
    "    wcols = [c for c in wcols if c in weather_rows.columns]\n",
    "    daily_wx = (\n",
    "        weather_rows.groupby([\"user_id\", \"checkin_date\"])[wcols].mean().reset_index()\n",
    "    )\n",
    "else:\n",
    "    daily_wx = pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "\n",
    "# ========= Combine into one daily table =========\n",
    "# Start from all user-day keys present in any table\n",
    "parts = [daily_cond, daily_sym, daily_food, daily_tag, daily_trt, daily_wx]\n",
    "daily = None\n",
    "for p in parts:\n",
    "    if p is None or p.empty: \n",
    "        continue\n",
    "    daily = p if daily is None else pd.merge(daily, p, on=[\"user_id\", \"checkin_date\"], how=\"outer\")\n",
    "\n",
    "if daily is None:\n",
    "    raise ValueError(\"No daily features built. Check your inputs.\")\n",
    "\n",
    "daily = daily.sort_values([\"user_id\", \"checkin_date\"]).reset_index(drop=True)\n",
    "daily = daily.fillna(0)  # for flags; numeric weather stays 0 if missing (fine for tree models)\n",
    "\n",
    "# ========= 6) ROLLING (PAST-ONLY) FEATURES =========\n",
    "# For each user, compute 7d/30d rolling sums of flags + rolling means of severities & weather.\n",
    "def add_rollups(g):\n",
    "    g = g.set_index(\"checkin_date\").sort_index()\n",
    "    # rolling windows (closed='left' to use ONLY past)\n",
    "    win_defs = {\"7d\":\"7D\", \"30d\":\"30D\"}\n",
    "    for col in g.columns:\n",
    "        if col.startswith((\"cond__\", \"sym__\", \"food__\", \"tag__\", \"trt__\")) and col.endswith(\"__max_severity\") is False:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__sum_{k}\"] = g[col].rolling(win, closed=\"left\").sum()\n",
    "        # severities & weather: rolling mean\n",
    "        if col.endswith(\"__max_severity\") or col in [\"temperature_min\",\"temperature_max\",\"precip_intensity\",\"pressure\",\"humidity\"]:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__mean_{k}\"] = g[col].rolling(win, closed=\"left\").mean()\n",
    "    return g.reset_index()\n",
    "\n",
    "daily = daily.groupby(\"user_id\", group_keys=False).apply(add_rollups)\n",
    "# Fill remaining NaNs from leading window edges\n",
    "daily = daily.fillna(0)\n",
    "\n",
    "# ========= 7) BUILD THE TARGET: FIRST-EVER EPILEPSY ONSET IN NEXT 30 DAYS =========\n",
    "# Find first date epilepsy is reported per user (Condition rows matched to epilepsy group)\n",
    "epi_flag_cols = [c for c in daily.columns if c.startswith(\"cond__epilepsy_seizure\")]\n",
    "if not epi_flag_cols:\n",
    "    raise ValueError(\"No epilepsy condition flag found. Ensure 'epilepsy_seizure' exists in keyword_groups for Conditions.\")\n",
    "\n",
    "# Daily presence of epilepsy (same-day)\n",
    "daily[\"has_epilepsy_today\"] = daily[epi_flag_cols].max(axis=1)\n",
    "\n",
    "# First-ever epilepsy date\n",
    "first_epi = (\n",
    "    daily[daily[\"has_epilepsy_today\"] > 0]\n",
    "    .groupby(\"user_id\", as_index=False)[\"checkin_date\"].min()\n",
    "    .rename(columns={\"checkin_date\":\"first_epilepsy_date\"})\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "daily = daily.merge(first_epi, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Labeling:\n",
    "# On a given day t (index row), y=1 if first epilepsy date falls in (t, t+30] AND user has not had epilepsy before t.\n",
    "daily[\"label_onset_30d\"] = 0\n",
    "mask_has_future = (~daily[\"first_epilepsy_date\"].isna())\n",
    "daily.loc[mask_has_future, \"label_onset_30d\"] = (\n",
    "    (daily[\"first_epilepsy_date\"] > daily[\"checkin_date\"]) &\n",
    "    (daily[\"first_epilepsy_date\"] <= daily[\"checkin_date\"] + pd.Timedelta(days=30))\n",
    ").astype(int)\n",
    "\n",
    "# Exclude rows ON/AFTER first epilepsy (were predicting onset, not after it happens)\n",
    "daily = daily[(daily[\"first_epilepsy_date\"].isna()) | (daily[\"checkin_date\"] < daily[\"first_epilepsy_date\"])]\n",
    "\n",
    "# Optional: require a minimum history window (e.g., at least 30 past days) to form features\n",
    "daily[\"has_30d_history\"] = daily.groupby(\"user_id\")[\"checkin_date\"].transform(\n",
    "    lambda s: (s - s.min()) >= pd.Timedelta(days=30)\n",
    ")\n",
    "daily = daily[daily[\"has_30d_history\"]]\n",
    "\n",
    "# ========= 8) ADD DEMOGRAPHICS (static) =========\n",
    "# Build a static per-user table (age/sex/country) at any row; then merge with daily\n",
    "demo = df.drop_duplicates(\"user_id\")[[\"user_id\",\"age\",\"sex\",\"country\"]].copy()\n",
    "daily = daily.merge(demo, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# One-hot encode sex & country (country can be many; consider top-K and bucket rest as 'other')\n",
    "# Keep top 20 countries to control dimensionality\n",
    "top_countries = df[\"country\"].value_counts().head(20).index\n",
    "daily[\"country_top\"] = daily[\"country\"].where(daily[\"country\"].isin(top_countries), \"other\")\n",
    "\n",
    "X_cat = pd.get_dummies(daily[[\"sex\",\"country_top\"]], drop_first=False, dtype=int)\n",
    "daily = pd.concat([daily.drop(columns=[\"sex\",\"country\",\"country_top\"]), X_cat], axis=1)\n",
    "\n",
    "# ========= 9) TRAIN / TEST TEMPORAL SPLIT =========\n",
    "# Choose a cutoff date (e.g., last year as test). Adjust to your range.\n",
    "cutoff = daily[\"checkin_date\"].quantile(0.8)  # 80% oldest for train, 20% most recent for test\n",
    "train = daily[daily[\"checkin_date\"] <= cutoff].copy()\n",
    "test  = daily[daily[\"checkin_date\"] >  cutoff].copy()\n",
    "\n",
    "# Features: use all engineered columns except identifiers and leakage columns\n",
    "drop_cols = {\n",
    "    \"user_id\", \"checkin_date\", \"first_epilepsy_date\",\n",
    "    \"has_epilepsy_today\", \"has_30d_history\", \"label_onset_30d\"\n",
    "}\n",
    "feature_cols = [c for c in daily.columns if c not in drop_cols]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"label_onset_30d\"].astype(int)\n",
    "X_test  = test[feature_cols]\n",
    "y_test  = test[\"label_onset_30d\"].astype(int)\n",
    "\n",
    "# Optional: scale numeric continuous columns (LightGBM doesnt require it)\n",
    "\n",
    "# ========= 10) TRAIN LIGHTGBM =========\n",
    "clf = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ========= 11) EVALUATE =========\n",
    "p_test = clf.predict_proba(X_test)[:,1]\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "auprc = average_precision_score(y_test, p_test)\n",
    "\n",
    "print(f\"AUC  : {auc:.3f}\")\n",
    "print(f\"AUPRC: {auprc:.3f}\")\n",
    "\n",
    "# Optional: show precision/recall at a few thresholds\n",
    "prec, rec, thr = precision_recall_curve(y_test, p_test)\n",
    "for t in [0.05, 0.10, 0.20]:\n",
    "    # nearest threshold\n",
    "    idx = (np.abs(thr - t)).argmin() if len(thr) else -1\n",
    "    if idx >= 0 and idx < len(prec):\n",
    "        print(f\"Threshold~{t:0.2f}: Precision={prec[idx]:.3f}  Recall={rec[idx]:.3f}\")\n",
    "\n",
    "# ========= 12) FEATURE IMPORTANCE =========\n",
    "imp = pd.Series(clf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"\\nTop 25 features:\\n\", imp.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb7fe5-5362-41ae-8c8b-814aa08995a9",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Forest Classifier\n",
    "\n",
    "### Predicting Onset of Epilepsy after 30 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d880c371-1941-46f6-8cc5-567bc6a560b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC  : 0.492\n",
      "AUPRC: 0.000\n",
      "Threshold~0.05: Precision=0.000  Recall=0.000\n",
      "Threshold~0.10: Precision=0.000  Recall=0.000\n",
      "Threshold~0.20: Precision=0.000  Recall=0.000\n",
      "\n",
      "Top 25 features:\n",
      " cond__ptsd__sum_30d               0.129790\n",
      "country_top_GB                    0.128462\n",
      "cond__ptsd                        0.118549\n",
      "cond__ptsd__sum_7d                0.100700\n",
      "age                               0.066054\n",
      "cond__depression__sum_30d         0.051636\n",
      "cond__depression                  0.049646\n",
      "cond__depression__sum_7d          0.049498\n",
      "cond__max_severity__mean_30d      0.043055\n",
      "country_top_US                    0.038558\n",
      "cond__max_severity__mean_7d       0.032084\n",
      "cond__insomnia__sum_30d           0.018474\n",
      "cond__chronic_fatigue__sum_30d    0.017087\n",
      "cond__max_severity                0.016337\n",
      "cond__chronic_fatigue__sum_7d     0.015345\n",
      "cond__anxiety__sum_30d            0.015025\n",
      "cond__anxiety__sum_7d             0.013213\n",
      "cond__anxiety                     0.011567\n",
      "cond__fibromyalgia__sum_7d        0.010141\n",
      "cond__insomnia                    0.007923\n",
      "cond__fibromyalgia__sum_30d       0.007546\n",
      "sex_other                         0.006891\n",
      "cond__chronic_fatigue             0.006849\n",
      "cond__migraine                    0.006189\n",
      "cond__migraine__sum_30d           0.006018\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 1) LOAD & CLEAN =========\n",
    "# Assumes your full long-format table is in df with the columns you listed\n",
    "path = \"/Users/cristybanuelos/Downloads/Chronic_Illness_Dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df[\"trackable_type\"] == \"Condition\"].copy()\n",
    "df[\"condition_clean\"] = (\n",
    "    df[\"trackable_name\"].str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Parse date\n",
    "df[\"checkin_date\"] = pd.to_datetime(df[\"checkin_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"checkin_date\"])  # keep only rows with a date\n",
    "\n",
    "# Basic user-level fields\n",
    "# Age cleaning: clip to a sensible window; set out-of-range to NaN, then impute\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "df.loc[(df[\"age\"] < 0) | (df[\"age\"] > 110), \"age\"] = np.nan  # clip biologically plausible ages\n",
    "df[\"age\"] = df.groupby(\"user_id\")[\"age\"].transform(lambda s: s.fillna(s.median()))\n",
    "df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "# Sex cleaning: normalize categories\n",
    "def norm_sex(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in {\"male\", \"m\"}: return \"male\"\n",
    "    if x in {\"female\", \"f\"}: return \"female\"\n",
    "    if x in {\"nan\", \"none\", \"\", \"unknown\"}: return \"unknown\"\n",
    "    return \"other\"\n",
    "df[\"sex\"] = df[\"sex\"].apply(norm_sex)\n",
    "\n",
    "# Country cleaning\n",
    "def norm_country(x):\n",
    "    x = str(x).strip()\n",
    "    return \"unknown\" if (x == \"\" or x.lower() == \"nan\") else x\n",
    "df[\"country\"] = df[\"country\"].apply(norm_country)\n",
    "\n",
    "# ========= 2) TEXT NORMALIZATION (for matching) =========\n",
    "# Create a clean text column for matching on trackable_name\n",
    "df[\"name_clean\"] = (\n",
    "    df[\"trackable_name\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# ========= 3) HELPERS TO FLAG KEYWORDS =========\n",
    "def add_keyword_flags(sub_df, groups_dict, prefix):\n",
    "    \"\"\"\n",
    "    For a subset of df (e.g., only Symptoms, only Food...), add binary columns\n",
    "    indicating whether trackable_name matches any keyword group for that row.\n",
    "    We then aggregate to daily features later.\n",
    "    \"\"\"\n",
    "    out = sub_df.copy()\n",
    "    for group, kws in groups_dict.items():\n",
    "        # Prebuild a regex OR pattern for speed; escape non-alnum safely\n",
    "        pattern = r\"(\" + \"|\".join([re.escape(k.lower()) for k in kws]) + r\")\"\n",
    "        col = f\"{prefix}__{group}\"\n",
    "        out[col] = out[\"name_clean\"].str.contains(pattern, regex=True).astype(int)\n",
    "    return out\n",
    "\n",
    "# ========= 4) PER-TYPE KEYWORD FLAGS =========\n",
    "# NOTE: You already have: keyword_groups (conditions incl. \"epilepsy_seizure\"),\n",
    "#       symptom_keyword_groups, food_keyword_groups, tag_keyword_groups, treatment_keyword_groups\n",
    "\n",
    "# Conditions (Condition rows only)\n",
    "cond_rows = df[df[\"trackable_type\"] == \"Condition\"]\n",
    "cond_rows = add_keyword_flags(cond_rows, condition_keyword_groups, \"cond\")\n",
    "\n",
    "# Symptoms\n",
    "sym_rows = df[df[\"trackable_type\"] == \"Symptom\"]\n",
    "sym_rows = add_keyword_flags(sym_rows, symptom_keyword_groups, \"sym\")\n",
    "\n",
    "# Food\n",
    "food_rows = df[df[\"trackable_type\"] == \"Food\"]\n",
    "food_rows = add_keyword_flags(food_rows, food_keyword_groups, \"food\")\n",
    "\n",
    "# Tags (triggers)\n",
    "tag_rows = df[df[\"trackable_type\"] == \"Tag\"]\n",
    "tag_rows = add_keyword_flags(tag_rows, tag_keyword_groups, \"tag\")\n",
    "\n",
    "# Treatments\n",
    "trt_rows = df[df[\"trackable_type\"] == \"Treatment\"]\n",
    "trt_rows = add_keyword_flags(trt_rows, treatment_keyword_groups, \"trt\")\n",
    "\n",
    "# Weather (already numeric columns; keep as-is if present)\n",
    "weather_rows = df[df[\"trackable_type\"] == \"Weather\"].copy()\n",
    "# Example expected numeric weather columns (adjust to your schema if needed)\n",
    "for col in [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]:\n",
    "    if col in weather_rows.columns:\n",
    "        weather_rows[col] = pd.to_numeric(weather_rows[col], errors=\"coerce\")\n",
    "\n",
    "# ========= 5) DAILY AGGREGATION (no leakage) =========\n",
    "# We build daily features per user, then later roll 7/30d windows that use ONLY past data.\n",
    "\n",
    "def daily_agg_flags(sub, prefix):\n",
    "    # get only the flag columns\n",
    "    flag_cols = [c for c in sub.columns if c.startswith(prefix + \"__\")]\n",
    "    if not flag_cols:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "    # include severity if available (0-4); well take max per day\n",
    "    if \"trackable_value\" in sub.columns:\n",
    "        sub[\"severity_val\"] = pd.to_numeric(sub[\"trackable_value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        sub[\"severity_val\"] = np.nan\n",
    "\n",
    "    agg = (\n",
    "        sub.groupby([\"user_id\", \"checkin_date\"])\n",
    "           .agg({**{c: \"max\" for c in flag_cols}, \"severity_val\": \"max\"})\n",
    "           .reset_index()\n",
    "    )\n",
    "    # rename severity\n",
    "    if \"severity_val\" in agg.columns:\n",
    "        agg = agg.rename(columns={\"severity_val\": f\"{prefix}__max_severity\"})\n",
    "    return agg\n",
    "\n",
    "daily_cond = daily_agg_flags(cond_rows,  \"cond\")\n",
    "daily_sym  = daily_agg_flags(sym_rows,   \"sym\")\n",
    "daily_food = daily_agg_flags(food_rows,  \"food\")\n",
    "daily_tag  = daily_agg_flags(tag_rows,   \"tag\")\n",
    "daily_trt  = daily_agg_flags(trt_rows,   \"trt\")\n",
    "\n",
    "# Daily weather (mean if multiple in same day)\n",
    "if not weather_rows.empty:\n",
    "    wcols = [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]\n",
    "    wcols = [c for c in wcols if c in weather_rows.columns]\n",
    "    daily_wx = (\n",
    "        weather_rows.groupby([\"user_id\", \"checkin_date\"])[wcols].mean().reset_index()\n",
    "    )\n",
    "else:\n",
    "    daily_wx = pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "\n",
    "# ========= Combine into one daily table =========\n",
    "# Start from all user-day keys present in any table\n",
    "parts = [daily_cond, daily_sym, daily_food, daily_tag, daily_trt, daily_wx]\n",
    "daily = None\n",
    "for p in parts:\n",
    "    if p is None or p.empty: \n",
    "        continue\n",
    "    daily = p if daily is None else pd.merge(daily, p, on=[\"user_id\", \"checkin_date\"], how=\"outer\")\n",
    "\n",
    "if daily is None:\n",
    "    raise ValueError(\"No daily features built. Check your inputs.\")\n",
    "\n",
    "daily = daily.sort_values([\"user_id\", \"checkin_date\"]).reset_index(drop=True)\n",
    "daily = daily.fillna(0)  # for flags; numeric weather stays 0 if missing (fine for tree models)\n",
    "\n",
    "# ========= 6) ROLLING (PAST-ONLY) FEATURES =========\n",
    "# For each user, compute 7d/30d rolling sums of flags + rolling means of severities & weather.\n",
    "def add_rollups(g):\n",
    "    g = g.set_index(\"checkin_date\").sort_index()\n",
    "    # rolling windows (closed='left' to use ONLY past)\n",
    "    win_defs = {\"7d\":\"7D\", \"30d\":\"30D\"}\n",
    "    for col in g.columns:\n",
    "        if col.startswith((\"cond__\", \"sym__\", \"food__\", \"tag__\", \"trt__\")) and col.endswith(\"__max_severity\") is False:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__sum_{k}\"] = g[col].rolling(win, closed=\"left\").sum()\n",
    "        # severities & weather: rolling mean\n",
    "        if col.endswith(\"__max_severity\") or col in [\"temperature_min\",\"temperature_max\",\"precip_intensity\",\"pressure\",\"humidity\"]:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__mean_{k}\"] = g[col].rolling(win, closed=\"left\").mean()\n",
    "    return g.reset_index()\n",
    "\n",
    "daily = daily.groupby(\"user_id\", group_keys=False).apply(add_rollups)\n",
    "# Fill remaining NaNs from leading window edges\n",
    "daily = daily.fillna(0)\n",
    "\n",
    "# ========= 7) BUILD THE TARGET: FIRST-EVER EPILEPSY ONSET IN NEXT 30 DAYS =========\n",
    "# Find first date epilepsy is reported per user (Condition rows matched to epilepsy group)\n",
    "epi_flag_cols = [c for c in daily.columns if c.startswith(\"cond__epilepsy_seizure\")]\n",
    "if not epi_flag_cols:\n",
    "    raise ValueError(\"No epilepsy condition flag found. Ensure 'epilepsy_seizure' exists in keyword_groups for Conditions.\")\n",
    "\n",
    "# Daily presence of epilepsy (same-day)\n",
    "daily[\"has_epilepsy_today\"] = daily[epi_flag_cols].max(axis=1)\n",
    "\n",
    "# First-ever epilepsy date\n",
    "first_epi = (\n",
    "    daily[daily[\"has_epilepsy_today\"] > 0]\n",
    "    .groupby(\"user_id\", as_index=False)[\"checkin_date\"].min()\n",
    "    .rename(columns={\"checkin_date\":\"first_epilepsy_date\"})\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "daily = daily.merge(first_epi, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Labeling:\n",
    "# On a given day t (index row), y=1 if first epilepsy date falls in (t, t+30] AND user has not had epilepsy before t.\n",
    "daily[\"label_onset_30d\"] = 0\n",
    "mask_has_future = (~daily[\"first_epilepsy_date\"].isna())\n",
    "daily.loc[mask_has_future, \"label_onset_30d\"] = (\n",
    "    (daily[\"first_epilepsy_date\"] > daily[\"checkin_date\"]) &\n",
    "    (daily[\"first_epilepsy_date\"] <= daily[\"checkin_date\"] + pd.Timedelta(days=30))\n",
    ").astype(int)\n",
    "\n",
    "# Exclude rows ON/AFTER first epilepsy (were predicting onset, not after it happens)\n",
    "daily = daily[(daily[\"first_epilepsy_date\"].isna()) | (daily[\"checkin_date\"] < daily[\"first_epilepsy_date\"])]\n",
    "\n",
    "# Optional: require a minimum history window (e.g., at least 30 past days) to form features\n",
    "daily[\"has_30d_history\"] = daily.groupby(\"user_id\")[\"checkin_date\"].transform(\n",
    "    lambda s: (s - s.min()) >= pd.Timedelta(days=30)\n",
    ")\n",
    "daily = daily[daily[\"has_30d_history\"]]\n",
    "\n",
    "# ========= 8) ADD DEMOGRAPHICS (static) =========\n",
    "# Build a static per-user table (age/sex/country) at any row; then merge with daily\n",
    "demo = df.drop_duplicates(\"user_id\")[[\"user_id\",\"age\",\"sex\",\"country\"]].copy()\n",
    "daily = daily.merge(demo, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# One-hot encode sex & country (country can be many; consider top-K and bucket rest as 'other')\n",
    "# Keep top 20 countries to control dimensionality\n",
    "top_countries = df[\"country\"].value_counts().head(20).index\n",
    "daily[\"country_top\"] = daily[\"country\"].where(daily[\"country\"].isin(top_countries), \"other\")\n",
    "\n",
    "X_cat = pd.get_dummies(daily[[\"sex\",\"country_top\"]], drop_first=False, dtype=int)\n",
    "daily = pd.concat([daily.drop(columns=[\"sex\",\"country\",\"country_top\"]), X_cat], axis=1)\n",
    "\n",
    "# ========= 9) TRAIN / TEST TEMPORAL SPLIT =========\n",
    "# Choose a cutoff date (e.g., last year as test). Adjust to your range.\n",
    "cutoff = daily[\"checkin_date\"].quantile(0.8)  # 80% oldest for train, 20% most recent for test\n",
    "train = daily[daily[\"checkin_date\"] <= cutoff].copy()\n",
    "test  = daily[daily[\"checkin_date\"] >  cutoff].copy()\n",
    "\n",
    "# Features: use all engineered columns except identifiers and leakage columns\n",
    "drop_cols = {\n",
    "    \"user_id\", \"checkin_date\", \"first_epilepsy_date\",\n",
    "    \"has_epilepsy_today\", \"has_30d_history\", \"label_onset_30d\"\n",
    "}\n",
    "feature_cols = [c for c in daily.columns if c not in drop_cols]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"label_onset_30d\"].astype(int)\n",
    "X_test  = test[feature_cols]\n",
    "y_test  = test[\"label_onset_30d\"].astype(int)\n",
    "\n",
    "# ========= 10) TRAIN RANDOM FOREST =========\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Keep only numeric columns\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test  = X_test.select_dtypes(include=[np.number])\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=300,        # number of trees in the forest\n",
    "    max_depth=None,          # let trees expand fully (or set a limit, e.g., 15)\n",
    "    min_samples_split=2,     # minimum samples required to split an internal node\n",
    "    min_samples_leaf=1,      # minimum samples required at a leaf node\n",
    "    max_features=\"sqrt\",     # number of features to consider at each split\n",
    "    class_weight=\"balanced\", # handle class imbalance\n",
    "    n_jobs=-1,               # use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ========= 11) EVALUATE =========\n",
    "p_test = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "auprc = average_precision_score(y_test, p_test)\n",
    "\n",
    "print(f\"AUC  : {auc:.3f}\")\n",
    "print(f\"AUPRC: {auprc:.3f}\")\n",
    "\n",
    "# Optional: show precision/recall at a few thresholds\n",
    "prec, rec, thr = precision_recall_curve(y_test, p_test)\n",
    "for t in [0.05, 0.10, 0.20]:\n",
    "    idx = (np.abs(thr - t)).argmin() if len(thr) else -1\n",
    "    if idx >= 0 and idx < len(prec):\n",
    "        print(f\"Threshold~{t:0.2f}: Precision={prec[idx]:.3f}  Recall={rec[idx]:.3f}\")\n",
    "\n",
    "# ========= 12) FEATURE IMPORTANCE =========\n",
    "imp = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 25 features:\\n\", imp.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b63201-8523-4aa9-b353-41e16793ade7",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b06531ba-39ac-4ac8-81fc-89fc1b388acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC  : 0.228\n",
      "AUPRC: 0.000\n",
      "Threshold~0.05: Precision=0.000  Recall=0.000\n",
      "Threshold~0.10: Precision=0.000  Recall=0.000\n",
      "Threshold~0.20: Precision=0.000  Recall=0.000\n",
      "\n",
      "Top 25 features (by absolute coefficient value):\n",
      "\n",
      "cond__insomnia__sum_7d          -7.720101\n",
      "cond__anxiety__sum_30d          -4.348590\n",
      "cond__chronic_fatigue           -2.944365\n",
      "country_top_GB                   2.718459\n",
      "cond__chronic_pain              -2.630666\n",
      "cond__joint_pain                -2.618889\n",
      "country_top_AU                  -2.602969\n",
      "cond__bipolar__sum_30d          -2.507664\n",
      "cond__insomnia__sum_30d          2.398315\n",
      "cond__anxiety__sum_7d            2.228293\n",
      "sex_male                        -2.210908\n",
      "cond__ptsd__sum_30d              2.089802\n",
      "cond__chronic_pain__sum_30d     -2.009180\n",
      "cond__max_severity__mean_7d     -1.971898\n",
      "cond__migraine__sum_30d         -1.823850\n",
      "sex_female                       1.726491\n",
      "cond__ocd                       -1.583430\n",
      "country_top_US                   1.475623\n",
      "cond__insomnia                   1.440051\n",
      "cond__bipolar__sum_7d           -1.364941\n",
      "cond__migraine__sum_7d           1.303991\n",
      "country_top_DE                  -1.291318\n",
      "country_top_CA                  -1.258839\n",
      "cond__fibromyalgia              -1.122990\n",
      "cond__chronic_fatigue__sum_7d    1.109618\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 1) LOAD & CLEAN =========\n",
    "# Assumes your full long-format table is in df with the columns you listed\n",
    "path = \"/Users/cristybanuelos/Downloads/Chronic_Illness_Dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df[\"trackable_type\"] == \"Condition\"].copy()\n",
    "df[\"condition_clean\"] = (\n",
    "    df[\"trackable_name\"].str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Parse date\n",
    "df[\"checkin_date\"] = pd.to_datetime(df[\"checkin_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"checkin_date\"])  # keep only rows with a date\n",
    "\n",
    "# Basic user-level fields\n",
    "# Age cleaning: clip to a sensible window; set out-of-range to NaN, then impute\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "df.loc[(df[\"age\"] < 0) | (df[\"age\"] > 110), \"age\"] = np.nan  # clip biologically plausible ages\n",
    "df[\"age\"] = df.groupby(\"user_id\")[\"age\"].transform(lambda s: s.fillna(s.median()))\n",
    "df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "# Sex cleaning: normalize categories\n",
    "def norm_sex(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in {\"male\", \"m\"}: return \"male\"\n",
    "    if x in {\"female\", \"f\"}: return \"female\"\n",
    "    if x in {\"nan\", \"none\", \"\", \"unknown\"}: return \"unknown\"\n",
    "    return \"other\"\n",
    "df[\"sex\"] = df[\"sex\"].apply(norm_sex)\n",
    "\n",
    "# Country cleaning\n",
    "def norm_country(x):\n",
    "    x = str(x).strip()\n",
    "    return \"unknown\" if (x == \"\" or x.lower() == \"nan\") else x\n",
    "df[\"country\"] = df[\"country\"].apply(norm_country)\n",
    "\n",
    "# ========= 2) TEXT NORMALIZATION (for matching) =========\n",
    "# Create a clean text column for matching on trackable_name\n",
    "df[\"name_clean\"] = (\n",
    "    df[\"trackable_name\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# ========= 3) HELPERS TO FLAG KEYWORDS =========\n",
    "def add_keyword_flags(sub_df, groups_dict, prefix):\n",
    "    \"\"\"\n",
    "    For a subset of df (e.g., only Symptoms, only Food...), add binary columns\n",
    "    indicating whether trackable_name matches any keyword group for that row.\n",
    "    We then aggregate to daily features later.\n",
    "    \"\"\"\n",
    "    out = sub_df.copy()\n",
    "    for group, kws in groups_dict.items():\n",
    "        # Prebuild a regex OR pattern for speed; escape non-alnum safely\n",
    "        pattern = r\"(\" + \"|\".join([re.escape(k.lower()) for k in kws]) + r\")\"\n",
    "        col = f\"{prefix}__{group}\"\n",
    "        out[col] = out[\"name_clean\"].str.contains(pattern, regex=True).astype(int)\n",
    "    return out\n",
    "\n",
    "# ========= 4) PER-TYPE KEYWORD FLAGS =========\n",
    "# NOTE: You already have: keyword_groups (conditions incl. \"epilepsy_seizure\"),\n",
    "#       symptom_keyword_groups, food_keyword_groups, tag_keyword_groups, treatment_keyword_groups\n",
    "\n",
    "# Conditions (Condition rows only)\n",
    "cond_rows = df[df[\"trackable_type\"] == \"Condition\"]\n",
    "cond_rows = add_keyword_flags(cond_rows, condition_keyword_groups, \"cond\")\n",
    "\n",
    "# Symptoms\n",
    "sym_rows = df[df[\"trackable_type\"] == \"Symptom\"]\n",
    "sym_rows = add_keyword_flags(sym_rows, symptom_keyword_groups, \"sym\")\n",
    "\n",
    "# Food\n",
    "food_rows = df[df[\"trackable_type\"] == \"Food\"]\n",
    "food_rows = add_keyword_flags(food_rows, food_keyword_groups, \"food\")\n",
    "\n",
    "# Tags (triggers)\n",
    "tag_rows = df[df[\"trackable_type\"] == \"Tag\"]\n",
    "tag_rows = add_keyword_flags(tag_rows, tag_keyword_groups, \"tag\")\n",
    "\n",
    "# Treatments\n",
    "trt_rows = df[df[\"trackable_type\"] == \"Treatment\"]\n",
    "trt_rows = add_keyword_flags(trt_rows, treatment_keyword_groups, \"trt\")\n",
    "\n",
    "# Weather (already numeric columns; keep as-is if present)\n",
    "weather_rows = df[df[\"trackable_type\"] == \"Weather\"].copy()\n",
    "# Example expected numeric weather columns (adjust to your schema if needed)\n",
    "for col in [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]:\n",
    "    if col in weather_rows.columns:\n",
    "        weather_rows[col] = pd.to_numeric(weather_rows[col], errors=\"coerce\")\n",
    "\n",
    "# ========= 5) DAILY AGGREGATION (no leakage) =========\n",
    "# We build daily features per user, then later roll 7/30d windows that use ONLY past data.\n",
    "\n",
    "def daily_agg_flags(sub, prefix):\n",
    "    # get only the flag columns\n",
    "    flag_cols = [c for c in sub.columns if c.startswith(prefix + \"__\")]\n",
    "    if not flag_cols:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "    # include severity if available (0-4); well take max per day\n",
    "    if \"trackable_value\" in sub.columns:\n",
    "        sub[\"severity_val\"] = pd.to_numeric(sub[\"trackable_value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        sub[\"severity_val\"] = np.nan\n",
    "\n",
    "    agg = (\n",
    "        sub.groupby([\"user_id\", \"checkin_date\"])\n",
    "           .agg({**{c: \"max\" for c in flag_cols}, \"severity_val\": \"max\"})\n",
    "           .reset_index()\n",
    "    )\n",
    "    # rename severity\n",
    "    if \"severity_val\" in agg.columns:\n",
    "        agg = agg.rename(columns={\"severity_val\": f\"{prefix}__max_severity\"})\n",
    "    return agg\n",
    "\n",
    "daily_cond = daily_agg_flags(cond_rows,  \"cond\")\n",
    "daily_sym  = daily_agg_flags(sym_rows,   \"sym\")\n",
    "daily_food = daily_agg_flags(food_rows,  \"food\")\n",
    "daily_tag  = daily_agg_flags(tag_rows,   \"tag\")\n",
    "daily_trt  = daily_agg_flags(trt_rows,   \"trt\")\n",
    "\n",
    "# Daily weather (mean if multiple in same day)\n",
    "if not weather_rows.empty:\n",
    "    wcols = [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]\n",
    "    wcols = [c for c in wcols if c in weather_rows.columns]\n",
    "    daily_wx = (\n",
    "        weather_rows.groupby([\"user_id\", \"checkin_date\"])[wcols].mean().reset_index()\n",
    "    )\n",
    "else:\n",
    "    daily_wx = pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "\n",
    "# ========= Combine into one daily table =========\n",
    "# Start from all user-day keys present in any table\n",
    "parts = [daily_cond, daily_sym, daily_food, daily_tag, daily_trt, daily_wx]\n",
    "daily = None\n",
    "for p in parts:\n",
    "    if p is None or p.empty: \n",
    "        continue\n",
    "    daily = p if daily is None else pd.merge(daily, p, on=[\"user_id\", \"checkin_date\"], how=\"outer\")\n",
    "\n",
    "if daily is None:\n",
    "    raise ValueError(\"No daily features built. Check your inputs.\")\n",
    "\n",
    "daily = daily.sort_values([\"user_id\", \"checkin_date\"]).reset_index(drop=True)\n",
    "daily = daily.fillna(0)  # for flags; numeric weather stays 0 if missing (fine for tree models)\n",
    "\n",
    "# ========= 6) ROLLING (PAST-ONLY) FEATURES =========\n",
    "# For each user, compute 7d/30d rolling sums of flags + rolling means of severities & weather.\n",
    "def add_rollups(g):\n",
    "    g = g.set_index(\"checkin_date\").sort_index()\n",
    "    # rolling windows (closed='left' to use ONLY past)\n",
    "    win_defs = {\"7d\":\"7D\", \"30d\":\"30D\"}\n",
    "    for col in g.columns:\n",
    "        if col.startswith((\"cond__\", \"sym__\", \"food__\", \"tag__\", \"trt__\")) and col.endswith(\"__max_severity\") is False:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__sum_{k}\"] = g[col].rolling(win, closed=\"left\").sum()\n",
    "        # severities & weather: rolling mean\n",
    "        if col.endswith(\"__max_severity\") or col in [\"temperature_min\",\"temperature_max\",\"precip_intensity\",\"pressure\",\"humidity\"]:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__mean_{k}\"] = g[col].rolling(win, closed=\"left\").mean()\n",
    "    return g.reset_index()\n",
    "\n",
    "daily = daily.groupby(\"user_id\", group_keys=False).apply(add_rollups)\n",
    "# Fill remaining NaNs from leading window edges\n",
    "daily = daily.fillna(0)\n",
    "\n",
    "# ========= 7) BUILD THE TARGET: FIRST-EVER EPILEPSY ONSET IN NEXT 30 DAYS =========\n",
    "# Find first date epilepsy is reported per user (Condition rows matched to epilepsy group)\n",
    "epi_flag_cols = [c for c in daily.columns if c.startswith(\"cond__epilepsy_seizure\")]\n",
    "if not epi_flag_cols:\n",
    "    raise ValueError(\"No epilepsy condition flag found. Ensure 'epilepsy_seizure' exists in keyword_groups for Conditions.\")\n",
    "\n",
    "# Daily presence of epilepsy (same-day)\n",
    "daily[\"has_epilepsy_today\"] = daily[epi_flag_cols].max(axis=1)\n",
    "\n",
    "# First-ever epilepsy date\n",
    "first_epi = (\n",
    "    daily[daily[\"has_epilepsy_today\"] > 0]\n",
    "    .groupby(\"user_id\", as_index=False)[\"checkin_date\"].min()\n",
    "    .rename(columns={\"checkin_date\":\"first_epilepsy_date\"})\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "daily = daily.merge(first_epi, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Labeling:\n",
    "# On a given day t (index row), y=1 if first epilepsy date falls in (t, t+30] AND user has not had epilepsy before t.\n",
    "daily[\"label_onset_30d\"] = 0\n",
    "mask_has_future = (~daily[\"first_epilepsy_date\"].isna())\n",
    "daily.loc[mask_has_future, \"label_onset_30d\"] = (\n",
    "    (daily[\"first_epilepsy_date\"] > daily[\"checkin_date\"]) &\n",
    "    (daily[\"first_epilepsy_date\"] <= daily[\"checkin_date\"] + pd.Timedelta(days=30))\n",
    ").astype(int)\n",
    "\n",
    "# Exclude rows ON/AFTER first epilepsy (were predicting onset, not after it happens)\n",
    "daily = daily[(daily[\"first_epilepsy_date\"].isna()) | (daily[\"checkin_date\"] < daily[\"first_epilepsy_date\"])]\n",
    "\n",
    "# Optional: require a minimum history window (e.g., at least 30 past days) to form features\n",
    "daily[\"has_30d_history\"] = daily.groupby(\"user_id\")[\"checkin_date\"].transform(\n",
    "    lambda s: (s - s.min()) >= pd.Timedelta(days=30)\n",
    ")\n",
    "daily = daily[daily[\"has_30d_history\"]]\n",
    "\n",
    "# ========= 8) ADD DEMOGRAPHICS (static) =========\n",
    "# Build a static per-user table (age/sex/country) at any row; then merge with daily\n",
    "demo = df.drop_duplicates(\"user_id\")[[\"user_id\",\"age\",\"sex\",\"country\"]].copy()\n",
    "daily = daily.merge(demo, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# One-hot encode sex & country (country can be many; consider top-K and bucket rest as 'other')\n",
    "# Keep top 20 countries to control dimensionality\n",
    "top_countries = df[\"country\"].value_counts().head(20).index\n",
    "daily[\"country_top\"] = daily[\"country\"].where(daily[\"country\"].isin(top_countries), \"other\")\n",
    "\n",
    "X_cat = pd.get_dummies(daily[[\"sex\",\"country_top\"]], drop_first=False, dtype=int)\n",
    "daily = pd.concat([daily.drop(columns=[\"sex\",\"country\",\"country_top\"]), X_cat], axis=1)\n",
    "\n",
    "# ========= 9) TRAIN / TEST TEMPORAL SPLIT =========\n",
    "# Choose a cutoff date (e.g., last year as test). Adjust to your range.\n",
    "cutoff = daily[\"checkin_date\"].quantile(0.8)  # 80% oldest for train, 20% most recent for test\n",
    "train = daily[daily[\"checkin_date\"] <= cutoff].copy()\n",
    "test  = daily[daily[\"checkin_date\"] >  cutoff].copy()\n",
    "\n",
    "# Features: use all engineered columns except identifiers and leakage columns\n",
    "drop_cols = {\n",
    "    \"user_id\", \"checkin_date\", \"first_epilepsy_date\",\n",
    "    \"has_epilepsy_today\", \"has_30d_history\", \"label_onset_30d\"\n",
    "}\n",
    "feature_cols = [c for c in daily.columns if c not in drop_cols]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"label_onset_30d\"].astype(int)\n",
    "X_test  = test[feature_cols]\n",
    "y_test  = test[\"label_onset_30d\"].astype(int)\n",
    "\n",
    "# ========= 10) TRAIN LOGISTIC REGRESSION =========\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Keep only numeric features\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test  = X_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# Build a pipeline: scale features  fit logistic regression\n",
    "clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(\n",
    "        penalty='l2',            # regularization (ridge)\n",
    "        solver='lbfgs',          # efficient for medium/large data\n",
    "        max_iter=1000,           # ensure convergence\n",
    "        class_weight='balanced', # handle class imbalance\n",
    "        n_jobs=-1,               # use all CPU cores\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ========= 11) EVALUATE =========\n",
    "p_test = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "auprc = average_precision_score(y_test, p_test)\n",
    "\n",
    "print(f\"AUC  : {auc:.3f}\")\n",
    "print(f\"AUPRC: {auprc:.3f}\")\n",
    "\n",
    "# Optional: show precision/recall at thresholds\n",
    "prec, rec, thr = precision_recall_curve(y_test, p_test)\n",
    "for t in [0.05, 0.10, 0.20]:\n",
    "    idx = (np.abs(thr - t)).argmin() if len(thr) else -1\n",
    "    if idx >= 0 and idx < len(prec):\n",
    "        print(f\"Threshold~{t:0.2f}: Precision={prec[idx]:.3f}  Recall={rec[idx]:.3f}\")\n",
    "\n",
    "# ========= 12) FEATURE IMPORTANCE (COEFFICIENTS) =========\n",
    "logreg_model = clf.named_steps['logreg']\n",
    "scaler = clf.named_steps['scaler']\n",
    "\n",
    "imp = pd.Series(\n",
    "    logreg_model.coef_[0],\n",
    "    index=X_train.columns\n",
    ").sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 25 features (by absolute coefficient value):\\n\")\n",
    "print(imp.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6917d3-30e7-44bb-b92c-a603df24354d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d3819733-df35-44fa-988a-31a14d63cb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train users: 5430, Test users: 1359\n",
      "Train positives: 63, Test positives: 53\n",
      "Train sequences: (63023, 30, 71)  Test sequences: (17670, 30, 71)\n",
      "Epoch 1/20 - Loss: 0.0421\n",
      "Epoch 2/20 - Loss: 0.0032\n",
      "Epoch 3/20 - Loss: 0.0025\n",
      "Epoch 4/20 - Loss: 0.0014\n",
      "Epoch 5/20 - Loss: 0.0011\n",
      "Epoch 6/20 - Loss: 0.0010\n",
      "Epoch 7/20 - Loss: 0.0011\n",
      "Epoch 8/20 - Loss: 0.0011\n",
      "Epoch 9/20 - Loss: 0.0009\n",
      "Epoch 10/20 - Loss: 0.0009\n",
      "Epoch 11/20 - Loss: 0.0007\n",
      "Epoch 12/20 - Loss: 0.0006\n",
      "Epoch 13/20 - Loss: 0.0005\n",
      "Epoch 14/20 - Loss: 0.0004\n",
      "Epoch 15/20 - Loss: 0.0003\n",
      "Epoch 16/20 - Loss: 0.0004\n",
      "Epoch 17/20 - Loss: 0.0003\n",
      "Epoch 18/20 - Loss: 0.0004\n",
      "Epoch 19/20 - Loss: 0.0003\n",
      "Epoch 20/20 - Loss: 0.0005\n",
      "AUC  : 1.000\n",
      "AUPRC: 0.998\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 1) LOAD & CLEAN =========\n",
    "# Assumes your full long-format table is in df with the columns you listed\n",
    "path = \"/Users/cristybanuelos/Downloads/Chronic_Illness_Dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df[\"trackable_type\"] == \"Condition\"].copy()\n",
    "df[\"condition_clean\"] = (\n",
    "    df[\"trackable_name\"].str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Parse date\n",
    "df[\"checkin_date\"] = pd.to_datetime(df[\"checkin_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"checkin_date\"])  # keep only rows with a date\n",
    "\n",
    "# Basic user-level fields\n",
    "# Age cleaning: clip to a sensible window; set out-of-range to NaN, then impute\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "df.loc[(df[\"age\"] < 0) | (df[\"age\"] > 110), \"age\"] = np.nan  # clip biologically plausible ages\n",
    "df[\"age\"] = df.groupby(\"user_id\")[\"age\"].transform(lambda s: s.fillna(s.median()))\n",
    "df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "# Sex cleaning: normalize categories\n",
    "def norm_sex(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in {\"male\", \"m\"}: return \"male\"\n",
    "    if x in {\"female\", \"f\"}: return \"female\"\n",
    "    if x in {\"nan\", \"none\", \"\", \"unknown\"}: return \"unknown\"\n",
    "    return \"other\"\n",
    "df[\"sex\"] = df[\"sex\"].apply(norm_sex)\n",
    "\n",
    "# Country cleaning\n",
    "def norm_country(x):\n",
    "    x = str(x).strip()\n",
    "    return \"unknown\" if (x == \"\" or x.lower() == \"nan\") else x\n",
    "df[\"country\"] = df[\"country\"].apply(norm_country)\n",
    "\n",
    "# ========= 2) TEXT NORMALIZATION (for matching) =========\n",
    "# Create a clean text column for matching on trackable_name\n",
    "df[\"name_clean\"] = (\n",
    "    df[\"trackable_name\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# ========= 3) HELPERS TO FLAG KEYWORDS =========\n",
    "def add_keyword_flags(sub_df, groups_dict, prefix):\n",
    "    \"\"\"\n",
    "    For a subset of df (e.g., only Symptoms, only Food...), add binary columns\n",
    "    indicating whether trackable_name matches any keyword group for that row.\n",
    "    We then aggregate to daily features later.\n",
    "    \"\"\"\n",
    "    out = sub_df.copy()\n",
    "    for group, kws in groups_dict.items():\n",
    "        # Prebuild a regex OR pattern for speed; escape non-alnum safely\n",
    "        pattern = r\"(\" + \"|\".join([re.escape(k.lower()) for k in kws]) + r\")\"\n",
    "        col = f\"{prefix}__{group}\"\n",
    "        out[col] = out[\"name_clean\"].str.contains(pattern, regex=True).astype(int)\n",
    "    return out\n",
    "\n",
    "# ========= 4) PER-TYPE KEYWORD FLAGS =========\n",
    "# NOTE: You already have: keyword_groups (conditions incl. \"epilepsy_seizure\"),\n",
    "#       symptom_keyword_groups, food_keyword_groups, tag_keyword_groups, treatment_keyword_groups\n",
    "\n",
    "# Conditions (Condition rows only)\n",
    "cond_rows = df[df[\"trackable_type\"] == \"Condition\"]\n",
    "cond_rows = add_keyword_flags(cond_rows, condition_keyword_groups, \"cond\")\n",
    "\n",
    "# Symptoms\n",
    "sym_rows = df[df[\"trackable_type\"] == \"Symptom\"]\n",
    "sym_rows = add_keyword_flags(sym_rows, symptom_keyword_groups, \"sym\")\n",
    "\n",
    "# Food\n",
    "food_rows = df[df[\"trackable_type\"] == \"Food\"]\n",
    "food_rows = add_keyword_flags(food_rows, food_keyword_groups, \"food\")\n",
    "\n",
    "# Tags (triggers)\n",
    "tag_rows = df[df[\"trackable_type\"] == \"Tag\"]\n",
    "tag_rows = add_keyword_flags(tag_rows, tag_keyword_groups, \"tag\")\n",
    "\n",
    "# Treatments\n",
    "trt_rows = df[df[\"trackable_type\"] == \"Treatment\"]\n",
    "trt_rows = add_keyword_flags(trt_rows, treatment_keyword_groups, \"trt\")\n",
    "\n",
    "# Weather (already numeric columns; keep as-is if present)\n",
    "weather_rows = df[df[\"trackable_type\"] == \"Weather\"].copy()\n",
    "# Example expected numeric weather columns (adjust to your schema if needed)\n",
    "for col in [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]:\n",
    "    if col in weather_rows.columns:\n",
    "        weather_rows[col] = pd.to_numeric(weather_rows[col], errors=\"coerce\")\n",
    "\n",
    "# ========= 5) DAILY AGGREGATION (no leakage) =========\n",
    "# We build daily features per user, then later roll 7/30d windows that use ONLY past data.\n",
    "\n",
    "def daily_agg_flags(sub, prefix):\n",
    "    # get only the flag columns\n",
    "    flag_cols = [c for c in sub.columns if c.startswith(prefix + \"__\")]\n",
    "    if not flag_cols:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "    # include severity if available (0-4); well take max per day\n",
    "    if \"trackable_value\" in sub.columns:\n",
    "        sub[\"severity_val\"] = pd.to_numeric(sub[\"trackable_value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        sub[\"severity_val\"] = np.nan\n",
    "\n",
    "    agg = (\n",
    "        sub.groupby([\"user_id\", \"checkin_date\"])\n",
    "           .agg({**{c: \"max\" for c in flag_cols}, \"severity_val\": \"max\"})\n",
    "           .reset_index()\n",
    "    )\n",
    "    # rename severity\n",
    "    if \"severity_val\" in agg.columns:\n",
    "        agg = agg.rename(columns={\"severity_val\": f\"{prefix}__max_severity\"})\n",
    "    return agg\n",
    "\n",
    "daily_cond = daily_agg_flags(cond_rows,  \"cond\")\n",
    "daily_sym  = daily_agg_flags(sym_rows,   \"sym\")\n",
    "daily_food = daily_agg_flags(food_rows,  \"food\")\n",
    "daily_tag  = daily_agg_flags(tag_rows,   \"tag\")\n",
    "daily_trt  = daily_agg_flags(trt_rows,   \"trt\")\n",
    "\n",
    "# Daily weather (mean if multiple in same day)\n",
    "if not weather_rows.empty:\n",
    "    wcols = [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]\n",
    "    wcols = [c for c in wcols if c in weather_rows.columns]\n",
    "    daily_wx = (\n",
    "        weather_rows.groupby([\"user_id\", \"checkin_date\"])[wcols].mean().reset_index()\n",
    "    )\n",
    "else:\n",
    "    daily_wx = pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "\n",
    "# ========= Combine into one daily table =========\n",
    "# Start from all user-day keys present in any table\n",
    "parts = [daily_cond, daily_sym, daily_food, daily_tag, daily_trt, daily_wx]\n",
    "daily = None\n",
    "for p in parts:\n",
    "    if p is None or p.empty: \n",
    "        continue\n",
    "    daily = p if daily is None else pd.merge(daily, p, on=[\"user_id\", \"checkin_date\"], how=\"outer\")\n",
    "\n",
    "if daily is None:\n",
    "    raise ValueError(\"No daily features built. Check your inputs.\")\n",
    "\n",
    "daily = daily.sort_values([\"user_id\", \"checkin_date\"]).reset_index(drop=True)\n",
    "daily = daily.fillna(0)  # for flags; numeric weather stays 0 if missing (fine for tree models)\n",
    "\n",
    "# ========= 6) ROLLING (PAST-ONLY) FEATURES =========\n",
    "# For each user, compute 7d/30d rolling sums of flags + rolling means of severities & weather.\n",
    "def add_rollups(g):\n",
    "    g = g.set_index(\"checkin_date\").sort_index()\n",
    "    # rolling windows (closed='left' to use ONLY past)\n",
    "    win_defs = {\"7d\":\"7D\", \"30d\":\"30D\"}\n",
    "    for col in g.columns:\n",
    "        if col.startswith((\"cond__\", \"sym__\", \"food__\", \"tag__\", \"trt__\")) and col.endswith(\"__max_severity\") is False:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__sum_{k}\"] = g[col].rolling(win, closed=\"left\").sum()\n",
    "        # severities & weather: rolling mean\n",
    "        if col.endswith(\"__max_severity\") or col in [\"temperature_min\",\"temperature_max\",\"precip_intensity\",\"pressure\",\"humidity\"]:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__mean_{k}\"] = g[col].rolling(win, closed=\"left\").mean()\n",
    "    return g.reset_index()\n",
    "\n",
    "daily = daily.groupby(\"user_id\", group_keys=False).apply(add_rollups)\n",
    "# Fill remaining NaNs from leading window edges\n",
    "daily = daily.fillna(0)\n",
    "\n",
    "# ========= 7) BUILD THE TARGET: FIRST-EVER EPILEPSY ONSET IN NEXT 30 DAYS =========\n",
    "# Find first date epilepsy is reported per user (Condition rows matched to epilepsy group)\n",
    "epi_flag_cols = [c for c in daily.columns if c.startswith(\"cond__epilepsy_seizure\")]\n",
    "if not epi_flag_cols:\n",
    "    raise ValueError(\"No epilepsy condition flag found. Ensure 'epilepsy_seizure' exists in keyword_groups for Conditions.\")\n",
    "\n",
    "# Daily presence of epilepsy (same-day)\n",
    "daily[\"has_epilepsy_today\"] = daily[epi_flag_cols].max(axis=1)\n",
    "\n",
    "# First-ever epilepsy date\n",
    "first_epi = (\n",
    "    daily[daily[\"has_epilepsy_today\"] > 0]\n",
    "    .groupby(\"user_id\", as_index=False)[\"checkin_date\"].min()\n",
    "    .rename(columns={\"checkin_date\":\"first_epilepsy_date\"})\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "daily = daily.merge(first_epi, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Labeling:\n",
    "# On a given day t (index row), y=1 if first epilepsy date falls in (t, t+30] AND user has not had epilepsy before t.\n",
    "daily[\"label_onset_30d\"] = 0\n",
    "mask_has_future = (~daily[\"first_epilepsy_date\"].isna())\n",
    "daily.loc[mask_has_future, \"label_onset_30d\"] = (\n",
    "    (daily[\"first_epilepsy_date\"] > daily[\"checkin_date\"]) &\n",
    "    (daily[\"first_epilepsy_date\"] <= daily[\"checkin_date\"] + pd.Timedelta(days=30))\n",
    ").astype(int)\n",
    "\n",
    "# Exclude rows ON/AFTER first epilepsy (were predicting onset, not after it happens)\n",
    "daily = daily[(daily[\"first_epilepsy_date\"].isna()) | (daily[\"checkin_date\"] < daily[\"first_epilepsy_date\"])]\n",
    "\n",
    "# Optional: require a minimum history window (e.g., at least 30 past days) to form features\n",
    "daily[\"has_30d_history\"] = daily.groupby(\"user_id\")[\"checkin_date\"].transform(\n",
    "    lambda s: (s - s.min()) >= pd.Timedelta(days=30)\n",
    ")\n",
    "daily = daily[daily[\"has_30d_history\"]]\n",
    "\n",
    "# ========= 8) ADD DEMOGRAPHICS (static) =========\n",
    "# Build a static per-user table (age/sex/country) at any row; then merge with daily\n",
    "demo = df.drop_duplicates(\"user_id\")[[\"user_id\",\"age\",\"sex\",\"country\"]].copy()\n",
    "daily = daily.merge(demo, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# One-hot encode sex & country (country can be many; consider top-K and bucket rest as 'other')\n",
    "# Keep top 20 countries to control dimensionality\n",
    "top_countries = df[\"country\"].value_counts().head(20).index\n",
    "daily[\"country_top\"] = daily[\"country\"].where(daily[\"country\"].isin(top_countries), \"other\")\n",
    "\n",
    "X_cat = pd.get_dummies(daily[[\"sex\",\"country_top\"]], drop_first=False, dtype=int)\n",
    "daily = pd.concat([daily.drop(columns=[\"sex\",\"country\",\"country_top\"]), X_cat], axis=1)\n",
    "\n",
    "# ========= 9) STRATIFIED TEMPORAL SPLIT =========\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Identify users who ever had an onset event (positive class)\n",
    "pos_users = daily.loc[daily[\"label_onset_30d\"] == 1, \"user_id\"].unique()\n",
    "neg_users = daily.loc[daily[\"label_onset_30d\"] == 0, \"user_id\"].unique()\n",
    "\n",
    "# Split users stratified by label  ensures positives in both sets\n",
    "train_pos_users, test_pos_users = train_test_split(pos_users, test_size=0.2, random_state=42)\n",
    "train_neg_users, test_neg_users = train_test_split(neg_users, test_size=0.2, random_state=42)\n",
    "\n",
    "train_users = np.concatenate([train_pos_users, train_neg_users])\n",
    "test_users  = np.concatenate([test_pos_users,  test_neg_users])\n",
    "\n",
    "train = daily[daily[\"user_id\"].isin(train_users)].copy()\n",
    "test  = daily[daily[\"user_id\"].isin(test_users)].copy()\n",
    "\n",
    "# Sort each user's timeline to preserve temporal order\n",
    "train = train.sort_values([\"user_id\", \"checkin_date\"])\n",
    "test  = test.sort_values([\"user_id\", \"checkin_date\"])\n",
    "\n",
    "print(f\"Train users: {train['user_id'].nunique()}, Test users: {test['user_id'].nunique()}\")\n",
    "print(f\"Train positives: {train['label_onset_30d'].sum()}, Test positives: {test['label_onset_30d'].sum()}\")\n",
    "\n",
    "# ========= Feature selection =========\n",
    "drop_cols = {\n",
    "    \"user_id\", \"checkin_date\", \"first_epilepsy_date\",\n",
    "    \"has_epilepsy_today\", \"has_30d_history\", \"label_onset_30d\"\n",
    "}\n",
    "feature_cols = [c for c in daily.columns if c not in drop_cols]\n",
    "\n",
    "X_train = train[feature_cols].select_dtypes(include=[np.number])\n",
    "y_train = train[\"label_onset_30d\"].astype(int)\n",
    "X_test  = test[feature_cols].select_dtypes(include=[np.number])\n",
    "y_test  = test[\"label_onset_30d\"].astype(int)\n",
    "\n",
    "# ========= 10) PREPARE SEQUENCES FOR RNN =========\n",
    "SEQ_LEN = 30  # use 30 days of history for each sample\n",
    "\n",
    "def build_sequences(df, feature_cols, seq_len=SEQ_LEN):\n",
    "    \"\"\"\n",
    "    Build sequences of daily features for each user with rolling 30-day history.\n",
    "    Returns arrays of shape (n_samples, seq_len, n_features)\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for uid, user_df in df.groupby(\"user_id\"):\n",
    "        user_df = user_df.sort_values(\"checkin_date\")\n",
    "        feats = user_df[feature_cols].values\n",
    "        labels = user_df[\"label_onset_30d\"].values\n",
    "        if len(user_df) >= seq_len:\n",
    "            for i in range(seq_len, len(user_df)):\n",
    "                X_seq.append(feats[i-seq_len:i])\n",
    "                y_seq.append(labels[i])\n",
    "    return np.stack(X_seq), np.array(y_seq)\n",
    "\n",
    "X_train_seq, y_train_seq = build_sequences(train, X_train.columns)\n",
    "X_test_seq,  y_test_seq  = build_sequences(test,  X_test.columns)\n",
    "\n",
    "print(\"Train sequences:\", X_train_seq.shape, \" Test sequences:\", X_test_seq.shape)\n",
    "\n",
    "# ========= Torch Dataset / Dataloader =========\n",
    "class HealthSeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = HealthSeqDataset(X_train_seq, y_train_seq)\n",
    "test_ds  = HealthSeqDataset(X_test_seq,  y_test_seq)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=128, shuffle=False)\n",
    "\n",
    "# ========= 11) DEFINE AND TRAIN LSTM MODEL =========\n",
    "class LSTMOnsetModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size=128, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(num_features, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]   # last time step output\n",
    "        return self.fc(out).squeeze()\n",
    "\n",
    "num_features = X_train_seq.shape[2]\n",
    "model = LSTMOnsetModel(num_features=num_features)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "# ========= 12) EVALUATE =========\n",
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(yb.numpy())\n",
    "\n",
    "p_test = np.array(all_preds)\n",
    "y_test_arr = np.array(all_true)\n",
    "\n",
    "# Safeguard: skip if test set has one class\n",
    "if len(np.unique(y_test_arr)) < 2:\n",
    "    print(\" Only one class present in test set  skipping AUC/AUPRC.\")\n",
    "else:\n",
    "    auc = roc_auc_score(y_test_arr, p_test)\n",
    "    auprc = average_precision_score(y_test_arr, p_test)\n",
    "    print(f\"AUC  : {auc:.3f}\")\n",
    "    print(f\"AUPRC: {auprc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c31d9-a117-47ce-906d-e3d1ba71d291",
   "metadata": {},
   "source": [
    "---\n",
    "### Temporal Convulational Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c483d33f-b5c4-49aa-abf4-e32290335e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train users: 5430, Test users: 1359\n",
      "Train positives: 63, Test positives: 53\n",
      "Train sequences: (63023, 30, 71)  Test sequences: (17670, 30, 71)\n",
      "Epoch 1/20 - Loss: 0.0490\n",
      "Epoch 2/20 - Loss: 0.0030\n",
      "Epoch 3/20 - Loss: 0.0025\n",
      "Epoch 4/20 - Loss: 0.0014\n",
      "Epoch 5/20 - Loss: 0.0010\n",
      "Epoch 6/20 - Loss: 0.0008\n",
      "Epoch 7/20 - Loss: 0.0007\n",
      "Epoch 8/20 - Loss: 0.0005\n",
      "Epoch 9/20 - Loss: 0.0005\n",
      "Epoch 10/20 - Loss: 0.0004\n",
      "Epoch 11/20 - Loss: 0.0003\n",
      "Epoch 12/20 - Loss: 0.0004\n",
      "Epoch 13/20 - Loss: 0.0003\n",
      "Epoch 14/20 - Loss: 0.0004\n",
      "Epoch 15/20 - Loss: 0.0002\n",
      "Epoch 16/20 - Loss: 0.0003\n",
      "Epoch 17/20 - Loss: 0.0003\n",
      "Epoch 18/20 - Loss: 0.0003\n",
      "Epoch 19/20 - Loss: 0.0003\n",
      "Epoch 20/20 - Loss: 0.0002\n",
      "AUC  : 1.000\n",
      "AUPRC: 0.996\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= 1) LOAD & CLEAN =========\n",
    "# Assumes your full long-format table is in df with the columns you listed\n",
    "path = \"/Users/cristybanuelos/Downloads/Chronic_Illness_Dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = df[df[\"trackable_type\"] == \"Condition\"].copy()\n",
    "df[\"condition_clean\"] = (\n",
    "    df[\"trackable_name\"].str.lower()\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    ")\n",
    "\n",
    "# Parse date\n",
    "df[\"checkin_date\"] = pd.to_datetime(df[\"checkin_date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"checkin_date\"])  # keep only rows with a date\n",
    "\n",
    "# Basic user-level fields\n",
    "# Age cleaning: clip to a sensible window; set out-of-range to NaN, then impute\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "df.loc[(df[\"age\"] < 0) | (df[\"age\"] > 110), \"age\"] = np.nan  # clip biologically plausible ages\n",
    "df[\"age\"] = df.groupby(\"user_id\")[\"age\"].transform(lambda s: s.fillna(s.median()))\n",
    "df[\"age\"] = df[\"age\"].fillna(df[\"age\"].median())\n",
    "\n",
    "# Sex cleaning: normalize categories\n",
    "def norm_sex(x):\n",
    "    x = str(x).strip().lower()\n",
    "    if x in {\"male\", \"m\"}: return \"male\"\n",
    "    if x in {\"female\", \"f\"}: return \"female\"\n",
    "    if x in {\"nan\", \"none\", \"\", \"unknown\"}: return \"unknown\"\n",
    "    return \"other\"\n",
    "df[\"sex\"] = df[\"sex\"].apply(norm_sex)\n",
    "\n",
    "# Country cleaning\n",
    "def norm_country(x):\n",
    "    x = str(x).strip()\n",
    "    return \"unknown\" if (x == \"\" or x.lower() == \"nan\") else x\n",
    "df[\"country\"] = df[\"country\"].apply(norm_country)\n",
    "\n",
    "# ========= 2) TEXT NORMALIZATION (for matching) =========\n",
    "# Create a clean text column for matching on trackable_name\n",
    "df[\"name_clean\"] = (\n",
    "    df[\"trackable_name\"]\n",
    "    .fillna(\"\")\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s\\-']\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# ========= 3) HELPERS TO FLAG KEYWORDS =========\n",
    "def add_keyword_flags(sub_df, groups_dict, prefix):\n",
    "    \"\"\"\n",
    "    For a subset of df (e.g., only Symptoms, only Food...), add binary columns\n",
    "    indicating whether trackable_name matches any keyword group for that row.\n",
    "    We then aggregate to daily features later.\n",
    "    \"\"\"\n",
    "    out = sub_df.copy()\n",
    "    for group, kws in groups_dict.items():\n",
    "        # Prebuild a regex OR pattern for speed; escape non-alnum safely\n",
    "        pattern = r\"(\" + \"|\".join([re.escape(k.lower()) for k in kws]) + r\")\"\n",
    "        col = f\"{prefix}__{group}\"\n",
    "        out[col] = out[\"name_clean\"].str.contains(pattern, regex=True).astype(int)\n",
    "    return out\n",
    "\n",
    "# ========= 4) PER-TYPE KEYWORD FLAGS =========\n",
    "# NOTE: You already have: keyword_groups (conditions incl. \"epilepsy_seizure\"),\n",
    "#       symptom_keyword_groups, food_keyword_groups, tag_keyword_groups, treatment_keyword_groups\n",
    "\n",
    "# Conditions (Condition rows only)\n",
    "cond_rows = df[df[\"trackable_type\"] == \"Condition\"]\n",
    "cond_rows = add_keyword_flags(cond_rows, condition_keyword_groups, \"cond\")\n",
    "\n",
    "# Symptoms\n",
    "sym_rows = df[df[\"trackable_type\"] == \"Symptom\"]\n",
    "sym_rows = add_keyword_flags(sym_rows, symptom_keyword_groups, \"sym\")\n",
    "\n",
    "# Food\n",
    "food_rows = df[df[\"trackable_type\"] == \"Food\"]\n",
    "food_rows = add_keyword_flags(food_rows, food_keyword_groups, \"food\")\n",
    "\n",
    "# Tags (triggers)\n",
    "tag_rows = df[df[\"trackable_type\"] == \"Tag\"]\n",
    "tag_rows = add_keyword_flags(tag_rows, tag_keyword_groups, \"tag\")\n",
    "\n",
    "# Treatments\n",
    "trt_rows = df[df[\"trackable_type\"] == \"Treatment\"]\n",
    "trt_rows = add_keyword_flags(trt_rows, treatment_keyword_groups, \"trt\")\n",
    "\n",
    "# Weather (already numeric columns; keep as-is if present)\n",
    "weather_rows = df[df[\"trackable_type\"] == \"Weather\"].copy()\n",
    "# Example expected numeric weather columns (adjust to your schema if needed)\n",
    "for col in [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]:\n",
    "    if col in weather_rows.columns:\n",
    "        weather_rows[col] = pd.to_numeric(weather_rows[col], errors=\"coerce\")\n",
    "\n",
    "# ========= 5) DAILY AGGREGATION (no leakage) =========\n",
    "# We build daily features per user, then later roll 7/30d windows that use ONLY past data.\n",
    "\n",
    "def daily_agg_flags(sub, prefix):\n",
    "    # get only the flag columns\n",
    "    flag_cols = [c for c in sub.columns if c.startswith(prefix + \"__\")]\n",
    "    if not flag_cols:\n",
    "        return pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "    # include severity if available (0-4); well take max per day\n",
    "    if \"trackable_value\" in sub.columns:\n",
    "        sub[\"severity_val\"] = pd.to_numeric(sub[\"trackable_value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        sub[\"severity_val\"] = np.nan\n",
    "\n",
    "    agg = (\n",
    "        sub.groupby([\"user_id\", \"checkin_date\"])\n",
    "           .agg({**{c: \"max\" for c in flag_cols}, \"severity_val\": \"max\"})\n",
    "           .reset_index()\n",
    "    )\n",
    "    # rename severity\n",
    "    if \"severity_val\" in agg.columns:\n",
    "        agg = agg.rename(columns={\"severity_val\": f\"{prefix}__max_severity\"})\n",
    "    return agg\n",
    "\n",
    "daily_cond = daily_agg_flags(cond_rows,  \"cond\")\n",
    "daily_sym  = daily_agg_flags(sym_rows,   \"sym\")\n",
    "daily_food = daily_agg_flags(food_rows,  \"food\")\n",
    "daily_tag  = daily_agg_flags(tag_rows,   \"tag\")\n",
    "daily_trt  = daily_agg_flags(trt_rows,   \"trt\")\n",
    "\n",
    "# Daily weather (mean if multiple in same day)\n",
    "if not weather_rows.empty:\n",
    "    wcols = [\"temperature_min\", \"temperature_max\", \"precip_intensity\", \"pressure\", \"humidity\"]\n",
    "    wcols = [c for c in wcols if c in weather_rows.columns]\n",
    "    daily_wx = (\n",
    "        weather_rows.groupby([\"user_id\", \"checkin_date\"])[wcols].mean().reset_index()\n",
    "    )\n",
    "else:\n",
    "    daily_wx = pd.DataFrame(columns=[\"user_id\", \"checkin_date\"])\n",
    "\n",
    "# ========= Combine into one daily table =========\n",
    "# Start from all user-day keys present in any table\n",
    "parts = [daily_cond, daily_sym, daily_food, daily_tag, daily_trt, daily_wx]\n",
    "daily = None\n",
    "for p in parts:\n",
    "    if p is None or p.empty: \n",
    "        continue\n",
    "    daily = p if daily is None else pd.merge(daily, p, on=[\"user_id\", \"checkin_date\"], how=\"outer\")\n",
    "\n",
    "if daily is None:\n",
    "    raise ValueError(\"No daily features built. Check your inputs.\")\n",
    "\n",
    "daily = daily.sort_values([\"user_id\", \"checkin_date\"]).reset_index(drop=True)\n",
    "daily = daily.fillna(0)  # for flags; numeric weather stays 0 if missing (fine for tree models)\n",
    "\n",
    "# ========= 6) ROLLING (PAST-ONLY) FEATURES =========\n",
    "# For each user, compute 7d/30d rolling sums of flags + rolling means of severities & weather.\n",
    "def add_rollups(g):\n",
    "    g = g.set_index(\"checkin_date\").sort_index()\n",
    "    # rolling windows (closed='left' to use ONLY past)\n",
    "    win_defs = {\"7d\":\"7D\", \"30d\":\"30D\"}\n",
    "    for col in g.columns:\n",
    "        if col.startswith((\"cond__\", \"sym__\", \"food__\", \"tag__\", \"trt__\")) and col.endswith(\"__max_severity\") is False:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__sum_{k}\"] = g[col].rolling(win, closed=\"left\").sum()\n",
    "        # severities & weather: rolling mean\n",
    "        if col.endswith(\"__max_severity\") or col in [\"temperature_min\",\"temperature_max\",\"precip_intensity\",\"pressure\",\"humidity\"]:\n",
    "            for k, win in win_defs.items():\n",
    "                g[f\"{col}__mean_{k}\"] = g[col].rolling(win, closed=\"left\").mean()\n",
    "    return g.reset_index()\n",
    "\n",
    "daily = daily.groupby(\"user_id\", group_keys=False).apply(add_rollups)\n",
    "# Fill remaining NaNs from leading window edges\n",
    "daily = daily.fillna(0)\n",
    "\n",
    "# ========= 7) BUILD THE TARGET: FIRST-EVER EPILEPSY ONSET IN NEXT 30 DAYS =========\n",
    "# Find first date epilepsy is reported per user (Condition rows matched to epilepsy group)\n",
    "epi_flag_cols = [c for c in daily.columns if c.startswith(\"cond__epilepsy_seizure\")]\n",
    "if not epi_flag_cols:\n",
    "    raise ValueError(\"No epilepsy condition flag found. Ensure 'epilepsy_seizure' exists in keyword_groups for Conditions.\")\n",
    "\n",
    "# Daily presence of epilepsy (same-day)\n",
    "daily[\"has_epilepsy_today\"] = daily[epi_flag_cols].max(axis=1)\n",
    "\n",
    "# First-ever epilepsy date\n",
    "first_epi = (\n",
    "    daily[daily[\"has_epilepsy_today\"] > 0]\n",
    "    .groupby(\"user_id\", as_index=False)[\"checkin_date\"].min()\n",
    "    .rename(columns={\"checkin_date\":\"first_epilepsy_date\"})\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "daily = daily.merge(first_epi, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Labeling:\n",
    "# On a given day t (index row), y=1 if first epilepsy date falls in (t, t+30] AND user has not had epilepsy before t.\n",
    "daily[\"label_onset_30d\"] = 0\n",
    "mask_has_future = (~daily[\"first_epilepsy_date\"].isna())\n",
    "daily.loc[mask_has_future, \"label_onset_30d\"] = (\n",
    "    (daily[\"first_epilepsy_date\"] > daily[\"checkin_date\"]) &\n",
    "    (daily[\"first_epilepsy_date\"] <= daily[\"checkin_date\"] + pd.Timedelta(days=30))\n",
    ").astype(int)\n",
    "\n",
    "# Exclude rows ON/AFTER first epilepsy (were predicting onset, not after it happens)\n",
    "daily = daily[(daily[\"first_epilepsy_date\"].isna()) | (daily[\"checkin_date\"] < daily[\"first_epilepsy_date\"])]\n",
    "\n",
    "# Optional: require a minimum history window (e.g., at least 30 past days) to form features\n",
    "daily[\"has_30d_history\"] = daily.groupby(\"user_id\")[\"checkin_date\"].transform(\n",
    "    lambda s: (s - s.min()) >= pd.Timedelta(days=30)\n",
    ")\n",
    "daily = daily[daily[\"has_30d_history\"]]\n",
    "\n",
    "# ========= 8) ADD DEMOGRAPHICS (static) =========\n",
    "# Build a static per-user table (age/sex/country) at any row; then merge with daily\n",
    "demo = df.drop_duplicates(\"user_id\")[[\"user_id\",\"age\",\"sex\",\"country\"]].copy()\n",
    "daily = daily.merge(demo, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# One-hot encode sex & country (country can be many; consider top-K and bucket rest as 'other')\n",
    "# Keep top 20 countries to control dimensionality\n",
    "top_countries = df[\"country\"].value_counts().head(20).index\n",
    "daily[\"country_top\"] = daily[\"country\"].where(daily[\"country\"].isin(top_countries), \"other\")\n",
    "\n",
    "X_cat = pd.get_dummies(daily[[\"sex\",\"country_top\"]], drop_first=False, dtype=int)\n",
    "daily = pd.concat([daily.drop(columns=[\"sex\",\"country\",\"country_top\"]), X_cat], axis=1)\n",
    "\n",
    "# ========= 9) STRATIFIED TEMPORAL SPLIT =========\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Identify users who ever had an onset event\n",
    "pos_users = daily.loc[daily[\"label_onset_30d\"] == 1, \"user_id\"].unique()\n",
    "neg_users = daily.loc[daily[\"label_onset_30d\"] == 0, \"user_id\"].unique()\n",
    "\n",
    "# Stratified split by user category (ensures positives in both train/test)\n",
    "train_pos_users, test_pos_users = train_test_split(pos_users, test_size=0.2, random_state=42)\n",
    "train_neg_users, test_neg_users = train_test_split(neg_users, test_size=0.2, random_state=42)\n",
    "\n",
    "train_users = np.concatenate([train_pos_users, train_neg_users])\n",
    "test_users  = np.concatenate([test_pos_users,  test_neg_users])\n",
    "\n",
    "train = daily[daily[\"user_id\"].isin(train_users)].copy()\n",
    "test  = daily[daily[\"user_id\"].isin(test_users)].copy()\n",
    "\n",
    "# Optional: enforce temporal order within each user's sequence\n",
    "def enforce_temporal_order(df):\n",
    "    out = []\n",
    "    for uid, user_df in df.groupby(\"user_id\"):\n",
    "        user_df = user_df.sort_values(\"checkin_date\")\n",
    "        out.append(user_df)\n",
    "    return pd.concat(out)\n",
    "\n",
    "train = enforce_temporal_order(train)\n",
    "test  = enforce_temporal_order(test)\n",
    "\n",
    "print(f\"Train users: {train['user_id'].nunique()}, Test users: {test['user_id'].nunique()}\")\n",
    "print(f\"Train positives: {train['label_onset_30d'].sum()}, Test positives: {test['label_onset_30d'].sum()}\")\n",
    "\n",
    "# ========= Feature selection =========\n",
    "drop_cols = {\n",
    "    \"user_id\", \"checkin_date\", \"first_epilepsy_date\",\n",
    "    \"has_epilepsy_today\", \"has_30d_history\", \"label_onset_30d\"\n",
    "}\n",
    "feature_cols = [c for c in daily.columns if c not in drop_cols]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"label_onset_30d\"].astype(int)\n",
    "X_test  = test[feature_cols]\n",
    "y_test  = test[\"label_onset_30d\"].astype(int)\n",
    "\n",
    "# ========= 10) PREPARE SEQUENCES FOR RNN =========\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Keep numeric features only\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test  = X_test.select_dtypes(include=[np.number])\n",
    "\n",
    "SEQ_LEN = 30  # use 30 days of history for each training example\n",
    "\n",
    "def build_sequences(df, feature_cols, seq_len=SEQ_LEN):\n",
    "    \"\"\"\n",
    "    Convert long user table into sequences of shape (num_users, seq_len, num_features)\n",
    "    using the last seq_len days before each cutoff date.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for uid, user_df in df.groupby(\"user_id\"):\n",
    "        user_df = user_df.sort_values(\"checkin_date\")\n",
    "        feats = user_df[feature_cols].values\n",
    "        labels = user_df[\"label_onset_30d\"].values\n",
    "        if len(user_df) >= seq_len:\n",
    "            for i in range(seq_len, len(user_df)):\n",
    "                X_seq.append(feats[i-seq_len:i])\n",
    "                y_seq.append(labels[i])\n",
    "    return np.stack(X_seq), np.array(y_seq)\n",
    "\n",
    "X_train_seq, y_train_seq = build_sequences(train, X_train.columns)\n",
    "X_test_seq,  y_test_seq  = build_sequences(test,  X_test.columns)\n",
    "\n",
    "print(\"Train sequences:\", X_train_seq.shape, \" Test sequences:\", X_test_seq.shape)\n",
    "\n",
    "# 2 Torch dataset -------------------------------------------------------\n",
    "class HealthSeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = HealthSeqDataset(X_train_seq, y_train_seq)\n",
    "test_ds  = HealthSeqDataset(X_test_seq,  y_test_seq)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=128, shuffle=False)\n",
    "\n",
    "# ========= 11) DEFINE AND TRAIN LSTM MODEL =========\n",
    "class LSTMOnsetModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size=128, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(num_features, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]   # last time step\n",
    "        return self.fc(out).squeeze()\n",
    "\n",
    "num_features = X_train_seq.shape[2]\n",
    "model = LSTMOnsetModel(num_features=num_features)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "# ========= 12) EVALUATE =========\n",
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(yb.numpy())\n",
    "\n",
    "p_test = np.array(all_preds)\n",
    "y_test_arr = np.array(all_true)\n",
    "\n",
    "# Skip AUC if test has only one class\n",
    "if len(np.unique(y_test_arr)) < 2:\n",
    "    print(\" Only one class in test set  skipping AUC/AUPRC computation.\")\n",
    "else:\n",
    "    auc = roc_auc_score(y_test_arr, p_test)\n",
    "    auprc = average_precision_score(y_test_arr, p_test)\n",
    "    print(f\"AUC  : {auc:.3f}\")\n",
    "    print(f\"AUPRC: {auprc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5382e9a-1899-443a-a5d9-788cde141fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
